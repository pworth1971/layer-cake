{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLNg_Puse6EX"
      },
      "source": [
        "In this notebook we will demonstrate different text classification models trained using the IMDB reviews dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOJLveJqtEO3",
        "outputId": "067a74b2-c5df-464d-a3fa-3f4517a9090a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n!pip install numpy==1.19.5\\n!pip install wget==3.2\\n!pip install tensorflow==1.14.0\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\"\"\"\n",
        "!pip install numpy==1.19.5\n",
        "!pip install wget==3.2\n",
        "!pip install tensorflow==1.14.0\n",
        "\"\"\"\n",
        "\n",
        "#!pip install numpy wget tensorflow tensorflow_datasets\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xqUcb7NBb5--"
      },
      "outputs": [],
      "source": [
        "#Make the necessary imports\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import wget\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MqW5vWwfiCP"
      },
      "source": [
        "Here we set all the paths of all the external datasets and models such as [glove](https://nlp.stanford.edu/projects/glove/) and [IMDB reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "MAX_WORDS = 10000\n",
        "\n",
        "# Load the IMDb dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=MAX_WORDS)\n",
        "\n",
        "# The data comes preprocessed as sequences of word indices.\n",
        "print(train_data[0])  # This will print an integer-encoded review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qvl1qb78fUib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GLOVE_DIR:  ../../layer-cake/.vector_cache/GloVe\n",
            "TRAIN_DATA_DIR:  ../datasets/IMDB//train\n",
            "TEST_DATA_DIR:  ../datasets/IMDB//test\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = '../../layer-cake/.vector_cache/'\n",
        "\n",
        "DATA_DIR = '../datasets/IMDB/'\n",
        "\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, 'GloVe')\n",
        "\n",
        "TRAIN_DATA_DIR = DATA_DIR + '/train'\n",
        "TEST_DATA_DIR = DATA_DIR + '/test'\n",
        "\n",
        "print(\"GLOVE_DIR: \", GLOVE_DIR)\n",
        "print(\"TRAIN_DATA_DIR: \", TRAIN_DATA_DIR)\n",
        "print(\"TEST_DATA_DIR: \", TEST_DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Yu9xmAZEd7fp"
      },
      "outputs": [],
      "source": [
        "#Within these, I only have a pos/ and a neg/ folder containing text files \n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "#started off from: https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
        "#and from: https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmifkoA8b5_N"
      },
      "source": [
        "### Loading and Preprocessing\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WI4O1usEb5_O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_texts[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "train_labels[0] 1\n",
            "test_texts[24999]: [1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 2, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643]\n",
            "test_labels[24999]: 0\n"
          ]
        }
      ],
      "source": [
        "#Function to load the data from the dataset into the notebook. Will be called twice - for train and test.\n",
        "\"\"\"\n",
        "def get_data(data_dir):\n",
        "    texts = []  # list of text samples\n",
        "    labels_index = {'pos':1, 'neg':0}  # dictionary mapping label name to numeric id\n",
        "    labels = []  # list of label ids\n",
        "    for name in sorted(os.listdir(data_dir)):\n",
        "        path = os.path.join(data_dir, name)\n",
        "        if os.path.isdir(path):\n",
        "            if name=='pos' or name=='neg':\n",
        "                label_id = labels_index[name]\n",
        "                for fname in sorted(os.listdir(path)):\n",
        "                        fpath = os.path.join(path, fname)\n",
        "                        text = open(fpath,encoding='utf8').read()\n",
        "                        texts.append(text)\n",
        "                        labels.append(label_id)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = get_data(TRAIN_DATA_DIR)\n",
        "test_texts, test_labels = get_data(TEST_DATA_DIR)\n",
        "\"\"\"\n",
        "\n",
        "train_texts, train_labels = train_data, train_labels\n",
        "test_texts, test_labels = test_data, test_labels\n",
        "\n",
        "labels_index = {'pos':1, 'neg':0} \n",
        "\n",
        "#Just to see how the data looks like. \n",
        "print(\"train_texts[0]:\", train_texts[0])\n",
        "print(\"train_labels[0]\", train_labels[0])\n",
        "\n",
        "print(\"test_texts[24999]:\", test_texts[24999])\n",
        "print(\"test_labels[24999]:\", test_labels[24999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-18 09:13:04.393163: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
            "0\n",
            "test_texts[24999]: They just don't make cartoons like they used to. This one had wit, great characters, and the greatest ensemble of voice over artists ever assembled for a daytime cartoon show. This still remains as one of the highest rated daytime cartoon shows, and one of the most honored, winning several Emmy Awards.\n",
            "test_labels[24999]: 1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the IMDb dataset\n",
        "imdb_data = tfds.load(\"imdb_reviews\", as_supervised=True)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = imdb_data['train'], imdb_data['test']\n",
        "\n",
        "# Extract the reviews and labels from the dataset (decode from bytes)\n",
        "train_texts = [text.decode('utf-8') for text, label in tfds.as_numpy(train_data)]\n",
        "test_texts = [text.decode('utf-8') for text, label in tfds.as_numpy(test_data)]\n",
        "\n",
        "# Extract the labels\n",
        "train_labels = [label for text, label in tfds.as_numpy(train_data)]\n",
        "test_labels = [label for text, label in tfds.as_numpy(test_data)]\n",
        "\n",
        "# Now, train_texts and test_texts can be fed into a tokenizer.\n",
        "print(train_texts[0])           # Example: First review in the training set\n",
        "print(train_labels[0])          # Example: Label of the first review in the training set\n",
        "\n",
        "print(\"test_texts[24999]:\", test_texts[24999])\n",
        "print(\"test_labels[24999]:\", test_labels[24999])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhqM0Jdd7fs",
        "outputId": "9b5b394e-bc52-4779-d85d-a0383446051d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer \n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data. \n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS) \n",
        "tokenizer.fit_on_texts(train_texts) \n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes \n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts) \n",
        "word_index = tokenizer.word_index \n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_sequences: <class 'list'> 25000\n",
            "train_sequences[0]: <class 'list'> 115\n",
            "train_sequences[0]: [11, 13, 32, 424, 391, 17, 89, 27, 10553, 8, 31, 1365, 3584, 39, 485, 11037, 196, 23, 84, 153, 18, 11, 212, 328, 27, 65, 246, 214, 8, 476, 57, 65, 84, 113, 97, 21, 5674, 11, 1321, 642, 766, 11, 17, 6, 32, 399, 8169, 175, 2454, 415, 1, 88, 1230, 136, 68, 145, 51, 1, 7576, 68, 228, 65, 2932, 15, 19499, 2903, 18510, 1478, 4939, 2, 38, 3899, 116, 1583, 16, 3584, 13, 161, 18, 3, 1230, 916, 7916, 8, 3, 17, 12, 13, 4138, 4, 98, 144, 1213, 10, 241, 682, 12, 47, 23, 99, 37, 11, 7180, 5514, 37, 1365, 13886, 49, 400, 10, 97, 1196, 866, 140, 9]\n"
          ]
        }
      ],
      "source": [
        "print(\"train_sequences:\", type(train_sequences), len(train_sequences))              #This is a list of lists, one list for each review\n",
        "print(\"train_sequences[0]:\", type(train_sequences[0]), len(train_sequences[0]))     #This is a list of word indexes for the first review\n",
        "print(\"train_sequences[0]:\", train_sequences[0])                                    #This will print a list of word indexes (depends on the tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_sequences: <class 'list'> 25000\n",
            "test_sequences[24999]: <class 'list'> 52\n",
            "test_sequences[24999]: [33, 40, 89, 94, 2465, 37, 33, 340, 5, 11, 28, 66, 2206, 84, 102, 2, 1, 830, 3143, 4, 541, 117, 2713, 123, 6520, 15, 3, 7607, 1069, 120, 11, 128, 1286, 14, 28, 4, 1, 4097, 1146, 7607, 1069, 284, 2, 28, 4, 1, 88, 14051, 1573, 447, 8607, 2126]\n"
          ]
        }
      ],
      "source": [
        "print(\"test_sequences:\", type(test_sequences), len(test_sequences))                               #This is a list of lists, one list for each review\n",
        "print(\"test_sequences[24999]:\", type(test_sequences[24999]), len(test_sequences[24999]))          #This is a list of word indexes for the 25000th review\n",
        "print(\"test_sequences[24999]:\", test_sequences[24999])                                            #This will print a list of word indexes (depends on the tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e0V1-bBb5_d",
        "outputId": "d866429d-5bb6-43a7-c66e-ed5abbafc4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ]
        }
      ],
      "source": [
        "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
        "#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
        "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "trainvalid_labels = to_categorical(np.asarray(train_labels))\n",
        "test_labels = to_categorical(np.asarray(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the training data into a training set and a validation set\n",
        "indices = np.arange(trainvalid_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "trainvalid_data = trainvalid_data[indices]\n",
        "trainvalid_labels = trainvalid_labels[indices]\n",
        "\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = trainvalid_data[:-num_validation_samples]\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\n",
        "x_val = trainvalid_data[-num_validation_samples:]\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\n",
        "\n",
        "#This is the data we will use for CNN and RNN training\n",
        "\n",
        "print('Splitting the train data into train and valid is done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUHqg2vvb5_l",
        "outputId": "8387eda1-18f0-4254-9819-e63191b8fc04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "GLOVE_MODEL:  glove.6B.100d.txt\n",
            "Found 400000 word vectors in Glove embeddings.\n",
            "Preparing of embedding matrix is done\n"
          ]
        }
      ],
      "source": [
        "print('Preparing embedding matrix.')\n",
        "\n",
        "GLOVE_MODEL = 'glove.6B.100d.txt'\n",
        "print(\"GLOVE_MODEL: \", GLOVE_MODEL)\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(GLOVE_DIR, GLOVE_MODEL),encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
        "#print(embeddings_index[\"google\"])\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_layer: <class 'keras.layers.core.embedding.Embedding'>\n",
            "embedding_layer: {'name': 'embedding', 'trainable': False, 'dtype': 'float32', 'batch_input_shape': (None, 1000), 'input_dim': 20001, 'output_dim': 100, 'embeddings_initializer': {'class_name': 'Constant', 'config': {'value': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ],\n",
            "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
            "         0.82779998,  0.27061999],\n",
            "       [-0.071953  ,  0.23127   ,  0.023731  , ..., -0.71894997,\n",
            "         0.86894   ,  0.19539   ],\n",
            "       ...,\n",
            "       [ 0.40121001, -0.6886    , -0.17046   , ..., -0.63893002,\n",
            "        -0.90948999, -0.69011003],\n",
            "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ],\n",
            "       [ 0.077072  , -0.1725    ,  0.20935   , ..., -0.24908   ,\n",
            "        -0.73106998,  0.13907   ]])}}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False, 'input_length': 1000}\n"
          ]
        }
      ],
      "source": [
        "print(\"embedding_layer:\", type(embedding_layer))\n",
        "print(\"embedding_layer:\", embedding_layer.get_config())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Function to detect and set the best available device\n",
        "def set_device():\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        print(\"Using GPU (CUDA)\")\n",
        "        return \"/device:GPU:0\"\n",
        "    elif tf.config.list_physical_devices('MPS'):\n",
        "        print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
        "        return \"/device:GPU:0\"  # MPS is identified as a GPU device in TensorFlow\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "        return \"/device:CPU:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEastnX8gdxR"
      },
      "source": [
        "### 1D CNN Model with pre-trained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTY-4K-Ob5_t",
        "outputId": "836681ca-936e-400a-8973-0754759bb7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU (CUDA)\n",
            "Running on device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Set the device\n",
        "device_name = set_device()\n",
        "print(\"Running on device:\", device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Define a 1D CNN model.\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 3s 18ms/step - loss: 0.6938 - acc: 0.6228 - val_loss: 0.4963 - val_acc: 0.7806\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.5058 - acc: 0.7607 - val_loss: 0.3944 - val_acc: 0.8246\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.4434 - acc: 0.8004 - val_loss: 0.4017 - val_acc: 0.8310\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.4590 - acc: 0.7818 - val_loss: 0.6630 - val_acc: 0.6016\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.4747 - acc: 0.7768 - val_loss: 0.3487 - val_acc: 0.8558\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.3687 - acc: 0.8380 - val_loss: 0.4880 - val_acc: 0.7920\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.3471 - acc: 0.8516 - val_loss: 0.3448 - val_acc: 0.8488\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.3148 - acc: 0.8698 - val_loss: 0.4526 - val_acc: 0.7880\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 3s 17ms/step - loss: 0.3142 - acc: 0.8733 - val_loss: 0.5263 - val_acc: 0.7996\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 3s 16ms/step - loss: 0.2848 - acc: 0.8862 - val_loss: 0.3480 - val_acc: 0.8536\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3662 - acc: 0.8460\n",
            "Test accuracy with CNN: 0.8460000157356262\n"
          ]
        }
      ],
      "source": [
        "print('Define a 1D CNN model.')\n",
        "\n",
        "with tf.device(device_name):\n",
        "\n",
        "    cnnmodel = Sequential()\n",
        "    cnnmodel.add(embedding_layer)\n",
        "    cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnnmodel.add(MaxPooling1D(5))\n",
        "    cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnnmodel.add(MaxPooling1D(5))\n",
        "    cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnnmodel.add(GlobalMaxPooling1D())\n",
        "    cnnmodel.add(Dense(128, activation='relu'))\n",
        "    cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "    cnnmodel.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "    \n",
        "    #Train the model. Tune to validation set. \n",
        "    cnnmodel.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "    #Evaluate on test set:\n",
        "    score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "    print('Test accuracy with CNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDj2FJzgi_W"
      },
      "source": [
        "### 1D CNN model with training your own embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_train: <class 'numpy.ndarray'> (20000, 2)\n",
            "y_train[0]: [0. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(\"y_train:\", type(y_train), y_train.shape)\n",
        "print(\"y_train[0]:\", y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train: <class 'numpy.ndarray'> (20000, 1000)\n",
            "x_train[0]: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0    11     6     3    52   218  1185    60    97\n",
            "    25    74   176   526  7393  1498  5714  1961   904     2   254     4\n",
            "    95  1498   231   786  1572     2   297  2122     5   984     3  2048\n",
            "     4    65  1095   254   343  5883     1  4583     4   786  1498   206\n",
            "   770    98  1027    10    59   132    12  3668   415     6     1  6820\n",
            "     8     1  6645     2  8071   343  2048  2848   445    20     3   280\n",
            "    62  1011     5    27    90    80     3   365   788    19    28    79\n",
            "  2498   134   146    24   343   135 12825     6    28     4     1   115\n",
            "   904     8     1   179   635   472    26   328     6     3  1298     4\n",
            "     1  3438    34    45    79     3  3326  1894     4     1   872   548\n",
            "   469    21    29  1498   528    23    90    14    70 14586 17921     8\n",
            "    24  2048  8004  7958     1  6257  7206     4     1   530  9808  6761\n",
            "  3245    60   415    97    25  4600    74     1    88   218    28    26\n",
            "   993  3604   343     6   859   395     2   906  2138    30   219   192\n",
            "     5   756    43  2917    82  1727  9499     4     1    19   463     1\n",
            "     8   486    10    59   128   199     1    19   690   155    15  1119\n",
            "    44    21    15  2600]\n"
          ]
        }
      ],
      "source": [
        "print(\"x_train:\", type(x_train), x_train.shape)\n",
        "print(\"x_train[0]:\", x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: /device:GPU:0\n",
            "Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 6s 38ms/step - loss: 0.5229 - acc: 0.7133 - val_loss: 0.2898 - val_acc: 0.8784\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.2625 - acc: 0.8936 - val_loss: 0.4531 - val_acc: 0.8252\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.1770 - acc: 0.9337 - val_loss: 0.4038 - val_acc: 0.8436\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.1224 - acc: 0.9577 - val_loss: 0.3077 - val_acc: 0.8762\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0743 - acc: 0.9757 - val_loss: 0.3657 - val_acc: 0.8812\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0429 - acc: 0.9871 - val_loss: 0.5828 - val_acc: 0.8716\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0270 - acc: 0.9915 - val_loss: 0.5233 - val_acc: 0.8838\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0200 - acc: 0.9951 - val_loss: 0.7719 - val_acc: 0.8830\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0189 - acc: 0.9958 - val_loss: 0.8647 - val_acc: 0.8826\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0196 - acc: 0.9963 - val_loss: 0.9934 - val_acc: 0.8732\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 1.0890 - acc: 0.8471\n",
            "Test accuracy with CNN: 0.8470799922943115\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using device: {device_name}\")\n",
        "\n",
        "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
        "\n",
        "# Define the CNN model\n",
        "cnnmodel = Sequential()\n",
        "\n",
        "# Force the Embedding layer to run on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "    cnnmodel.add(Embedding(MAX_NUM_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "# Rest of the model can run on the GPU\n",
        "with tf.device(device_name):\n",
        "    cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnnmodel.add(MaxPooling1D(5))\n",
        "    cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnnmodel.add(MaxPooling1D(5))\n",
        "    cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnnmodel.add(GlobalMaxPooling1D())\n",
        "    cnnmodel.add(Dense(128, activation='relu'))\n",
        "    cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
        "\n",
        "    cnnmodel.compile(loss='categorical_crossentropy',\n",
        "                     optimizer='rmsprop',\n",
        "                     metrics=['acc'])\n",
        "\n",
        "    #Train the model. Tune to validation set. \n",
        "    cnnmodel.fit(x_train, y_train,\n",
        "            batch_size=128,\n",
        "            epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "    #Evaluate on test set:\n",
        "    score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
        "    print('Test accuracy with CNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GwhXpmSgt4H"
      },
      "source": [
        "### LSTM Model with training your own embedding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBt2Brib5_4",
        "outputId": "008fe9fa-13bf-4127-ba46-67916426ddbe"
      },
      "outputs": [],
      "source": [
        "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
        "\n",
        "# Define the RNN model\n",
        "rnnmodel = Sequential()\n",
        "\n",
        "# Force the Embedding layer to run on the CPU\n",
        "with tf.device('/CPU:0'):\n",
        "        rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "\n",
        "with tf.device(device_name):\n",
        "        rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "        rnnmodel.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel.fit(x_train, y_train,\n",
        "        batch_size=128,\n",
        "        epochs=10,\n",
        "        validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
        "                        batch_size=32)\n",
        "\n",
        "print('Test accuracy with RNN:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJYzsZFSg9z-"
      },
      "source": [
        "### LSTM Model using pre-trained Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eymx0IyCb5_-",
        "outputId": "da0fa303-a4c4-4b92-ff42-54f1a1d51e45"
      },
      "outputs": [],
      "source": [
        "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
        "\n",
        "rnnmodel2 = Sequential()\n",
        "rnnmodel2.add(embedding_layer)\n",
        "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel2.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Training the RNN')\n",
        "\n",
        "rnnmodel2.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "          validation_data=(x_val, y_val))\n",
        "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "05_DeepNN_Example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
