{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCFWazYMSLJz",
        "outputId": "a4ab08eb-1487-475e-c7a8-3e5b1dd6f7ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/peter/miniconda3/lib/python3.12/site-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/peter/miniconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/peter/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/peter/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /Users/peter/miniconda3/lib/python3.12/site-packages (from pytorch-transformers) (2.4.0)\n",
            "Requirement already satisfied: numpy in /Users/peter/miniconda3/lib/python3.12/site-packages (from pytorch-transformers) (1.26.4)\n",
            "Collecting boto3 (from pytorch-transformers)\n",
            "  Downloading boto3-1.35.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: requests in /Users/peter/miniconda3/lib/python3.12/site-packages (from pytorch-transformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /Users/peter/miniconda3/lib/python3.12/site-packages (from pytorch-transformers) (4.66.4)\n",
            "Requirement already satisfied: regex in /Users/peter/miniconda3/lib/python3.12/site-packages (from pytorch-transformers) (2023.10.3)\n",
            "Requirement already satisfied: sentencepiece in /Users/peter/miniconda3/lib/python3.12/site-packages (from pytorch-transformers) (0.2.0)\n",
            "Collecting sacremoses (from pytorch-transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: filelock in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (2024.5.0)\n",
            "Requirement already satisfied: setuptools in /Users/peter/miniconda3/lib/python3.12/site-packages (from torch>=1.0.0->pytorch-transformers) (70.1.0)\n",
            "Collecting botocore<1.36.0,>=1.35.0 (from boto3->pytorch-transformers)\n",
            "  Downloading botocore-1.35.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->pytorch-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->pytorch-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->pytorch-transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/peter/miniconda3/lib/python3.12/site-packages (from requests->pytorch-transformers) (2024.7.4)\n",
            "Requirement already satisfied: click in /Users/peter/miniconda3/lib/python3.12/site-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/peter/miniconda3/lib/python3.12/site-packages (from sacremoses->pytorch-transformers) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/peter/miniconda3/lib/python3.12/site-packages (from botocore<1.36.0,>=1.35.0->boto3->pytorch-transformers) (2.9.0.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/peter/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/peter/miniconda3/lib/python3.12/site-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/peter/miniconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.0->boto3->pytorch-transformers) (1.16.0)\n",
            "Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "Downloading boto3-1.35.0-py3-none-any.whl (139 kB)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.0-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "Installing collected packages: sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.35.0 botocore-1.35.0 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.10.2 sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "09TscoDsSPpL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import math\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import tensorflow as tf\n",
        "#import tensorflow.keras.backend as K\n",
        "import tokenizers\n",
        "#from transformers import BertTokenizer, TFBertModel, BertForSequenceClassification, BertConfig\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7mM99kOWSZI2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on CPU\n",
            "Number of GPUs: 0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f'Running on GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Running on CPU')\n",
        "\n",
        "# Number of GPUs available\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print('Number of GPUs:', num_gpus)\n",
        "\n",
        "# If using multiple GPUs, use DataParallel or DistributedDataParallel\n",
        "if num_gpus > 1:\n",
        "    model = torch.nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4fGeJVk6SdKT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "Model: bert-base-uncased, Max Length: 256, Batch Size: 8, Epochs: 3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "ARTIFACTS_PATH = '../artifacts/'\n",
        "\n",
        "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "EPOCHS = 3\n",
        "\n",
        "if not os.path.exists(ARTIFACTS_PATH):\n",
        "    os.makedirs(ARTIFACTS_PATH)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "ARTIFACTS_PATH = '../../artifacts/'\n",
        "\n",
        "# Determine the number of devices (GPUs/CPUs)\n",
        "if torch.cuda.is_available():\n",
        "    num_replicas = torch.cuda.device_count()\n",
        "    print(f'Using {num_replicas} GPU(s)')\n",
        "else:\n",
        "    num_replicas = 1  # Use CPU or single GPU\n",
        "    print('Using CPU')\n",
        "\n",
        "# Set the batch size and number of epochs\n",
        "BATCH_SIZE = 8 * num_replicas\n",
        "EPOCHS = 3\n",
        "\n",
        "# Create the artifacts directory if it doesn't exist\n",
        "if not os.path.exists(ARTIFACTS_PATH):\n",
        "    os.makedirs(ARTIFACTS_PATH)\n",
        "\n",
        "print(f'Model: {MODEL_NAME}, Max Length: {MAX_LEN}, Batch Size: {BATCH_SIZE}, Epochs: {EPOCHS}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import model_selection, svm, metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, hamming_loss, jaccard_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from pprint import pprint\n",
        "import argparse\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import cufflinks as cf\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#\n",
        "# we assume everything runs from bin directory\n",
        "#\n",
        "PICKLE_DIR = '../../pickles/'\n",
        "OUT_DIR = '../../out/'\n",
        "DATASET_DIR = '../../datasets/'\n",
        "\n",
        "\n",
        "#dataset_available = {'reuters21578', '20newsgroups', 'ohsumed', 'rcv1'}\n",
        "dataset_available = {'20newsgroups', 'bbc-news'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def create_confusion_matrix(y_test, y_pred, title, file_name=OUT_DIR+'svm_20newsgroups_confusion_matrix_best_model_table.png', debug=False):\n",
        "\n",
        "    print(\"Creating confusion matrix...\")\n",
        "\n",
        "    # Assuming y_test and y_pred_best are already defined\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Plotting the confusion matrix as a table with numbers\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))  # Increase the width and height of the figure\n",
        "\n",
        "    # Hide axes\n",
        "    ax.xaxis.set_visible(False) \n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.set_frame_on(False)\n",
        "\n",
        "    # Create the table with smaller font sizes and adjusted scale\n",
        "    table = ax.table(\n",
        "        cellText=conf_matrix,\n",
        "        rowLabels=[f'Actual {i}' for i in range(conf_matrix.shape[0])],\n",
        "        colLabels=[f'Predicted {i}' for i in range(conf_matrix.shape[1])],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "\n",
        "    # Adjust the font size and layout\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)  # Reduced font size for better fitting\n",
        "    table.scale(1.2, 1.2)\n",
        "\n",
        "    # Add a title with centered text\n",
        "    plt.title(title, fontsize=16, pad=20)\n",
        "\n",
        "    # Adjust layout to add more padding around the plot\n",
        "    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)  # Increase padding on the left\n",
        "\n",
        "    # Save the plot to a file\n",
        "    confusion_matrix_filename = file_name\n",
        "    plt.savefig(confusion_matrix_filename, bbox_inches='tight')  # Ensure everything is saved in the output file\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Confusion matrix saved as {confusion_matrix_filename}\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Plain text explanation of the confusion matrix\n",
        "    if debug:\n",
        "        print(\"\\nHow to read this confusion matrix:\")\n",
        "        print(\"------------------------------------------------------\")\n",
        "        print(\"The confusion matrix shows the performance of the classification model.\")\n",
        "        print(\"Each row of the matrix represents the actual classes, while each column represents the predicted classes.\")\n",
        "        print(\"Values on the diagonal (from top-left to bottom-right) represent correct predictions (true positives and true negatives).\")\n",
        "        print(\"Values outside the diagonal represent incorrect predictions (false positives and false negatives).\")\n",
        "        print(\"\\nAccuracy Score: {:.2f}%\".format(accuracy * 100))\n",
        "        \n",
        "        print(\"\\nConfusion Matrix Values:\")\n",
        "        for i in range(len(conf_matrix)):\n",
        "            print(f\"Actual class {i}:\")\n",
        "            for j in range(len(conf_matrix[i])):\n",
        "                print(f\"  Predicted as class {j}: {conf_matrix[i][j]}\")\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#\n",
        "# Utility functions for preprocessing data\n",
        "#\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "def missing_values(df):\n",
        "    \"\"\"\n",
        "    Calculate the percentage of missing values for each column in a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "    df (pd.DataFrame): The input DataFrame to analyze.\n",
        "    \n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing the total count and percentage of missing values for each column.\n",
        "    \"\"\"\n",
        "    # Calculate total missing values and their percentage\n",
        "    total = df.isnull().sum()\n",
        "    percent = (total / len(df) * 100)\n",
        "    \n",
        "    # Create a DataFrame with the results\n",
        "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    \n",
        "    # Sort the DataFrame by percentage of missing values (descending)\n",
        "    missing_data = missing_data.sort_values('Percent', ascending=False)\n",
        "    \n",
        "    # Filter out columns with no missing values\n",
        "    missing_data = missing_data[missing_data['Total'] > 0]\n",
        "    \n",
        "    print(\"Columns with missing values:\")\n",
        "    print(missing_data)\n",
        "    \n",
        "    return missing_data\n",
        "\n",
        "\n",
        "def remove_punctuation(x):\n",
        "    punctuationfree=\"\".join([i for i in x if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "\n",
        "# Function to lemmatize text with memory optimization\n",
        "def lemmatization(texts, chunk_size=1000):\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    \n",
        "    num_chunks = len(texts) // chunk_size + 1\n",
        "    #print(f\"Number of chunks: {num_chunks}\")\n",
        "    for i in range(num_chunks):\n",
        "        chunk = texts[i*chunk_size:(i+1)*chunk_size]\n",
        "        texts[i*chunk_size:(i+1)*chunk_size] = [' '.join([lmtzr.lemmatize(word) for word in text.split()]) for text in chunk]\n",
        "    \n",
        "    return texts\n",
        "\n",
        "\n",
        "def preprocessDataset(train_text):\n",
        "    # Ensure input is string\n",
        "    train_text = str(train_text)\n",
        "    \n",
        "    # Word tokenization using NLTK's word_tokenize\n",
        "    tokenized_train_set = word_tokenize(train_text.lower())\n",
        "    \n",
        "    # Stop word removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stopwordremove = [i for i in tokenized_train_set if i not in stop_words]\n",
        "    \n",
        "    # Join words into sentence\n",
        "    stopwordremove_text = ' '.join(stopwordremove)\n",
        "    \n",
        "    # Remove numbers\n",
        "    numberremove_text = ''.join(c for c in stopwordremove_text if not c.isdigit())\n",
        "    \n",
        "    # Stemming using NLTK's PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stem_input = word_tokenize(numberremove_text)\n",
        "    stem_text = ' '.join([stemmer.stem(word) for word in stem_input])\n",
        "    \n",
        "    # Lemmatization using NLTK's WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    def get_wordnet_pos(word):\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "    \n",
        "    lem_input = word_tokenize(stem_text)\n",
        "    lem_text = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in lem_input])\n",
        "    \n",
        "    return lem_text\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def load_data(dataset='20newsgroups'):\n",
        "\n",
        "    print(f\"Loading data set {dataset}...\")\n",
        "\n",
        "    if (dataset == '20newsgroups'):\n",
        "        \n",
        "        print(\"Loading 20 newsgroups dataset...\")\n",
        "\n",
        "        # Fetch the 20 newsgroups dataset\n",
        "        newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "        # Create a DataFrame from the Bunch object\n",
        "        df = pd.DataFrame({\n",
        "            'text': newsgroups.data,\n",
        "            'category': newsgroups.target\n",
        "        })\n",
        "\n",
        "        # Add category names\n",
        "        df['category_name'] = [newsgroups.target_names[i] for i in df['category']]\n",
        "\n",
        "        print(f\"Number of documents: {len(df)}\")\n",
        "        print(f\"Number of categories: {len(df['category'].unique())}\")\n",
        "        print(f\"Number of category names: {len(df['category_name'].unique())}\")\n",
        "        #pprint(list(df.target_names))\n",
        "        #pprint(list(df.category_name))\n",
        "\n",
        "        #df = df['category'].unique()    \n",
        "\n",
        "        missing_values_df = missing_values(df)\n",
        "        print(f\"missing values:\", missing_values_df)\n",
        "\n",
        "        ### Start of Text Pre-processing\n",
        "        print(\"preproccessing...\")\n",
        "\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('averaged_perceptron_tagger')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('omw-1.4')\n",
        "\n",
        "        string.punctuation\n",
        "\n",
        "        ### 2. To LowerCase\n",
        "\n",
        "        df['CleanedText'] = (df.text.apply(lambda x: x.lower()))\n",
        "\n",
        "        ### 3. Removing Numbers and Special Characters including XXXXXX\n",
        "\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('\\W+', ' ', x)))\n",
        "        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
        "\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub(regex, '', x)))\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('xxxx', '', x)))\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('xx', '', x)))\n",
        "\n",
        "        print(\"removing punctuation...\")\n",
        "\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: remove_punctuation(x)))\n",
        "\n",
        "        ### 5. Tokenization\n",
        "        #data['TokenizedText'] =  (data.CleanedText.apply(lambda x: re.split('W+',x)))\n",
        "\n",
        "        print(\"removing stopwords...\")\n",
        "        from nltk.corpus import stopwords\n",
        "        nltk.download('stopwords')\n",
        "        stopwords = set(stopwords.words(\"english\"))\n",
        "        df['CleanedText'] = df.CleanedText.apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n",
        "        print(\"Stopwords removed\")\n",
        "        #print(df['CleanedText'][0])\n",
        "\n",
        "        ## TFIDF already tokenizes the text so no need to tokenize it here\n",
        "        # from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "        # data2['TokenizedText'] = data2.CleanedText.apply(word_tokenize)\n",
        "\n",
        "\n",
        "        ### 7. Text Normalization  [Lemmatization] -->better than Stemming since it returns actual words\n",
        "        ## lemmatization is an intelligent operation that uses dictionaries\n",
        "\n",
        "        print(\"Lemmatizing...\")\n",
        "\n",
        "        df['LemmatizedText'] = lemmatization(df['CleanedText'])\n",
        "\n",
        "        print(\"Lemmatized\")\n",
        "        #print(df['CleanedText'][0])\n",
        "        #print(df['LemmatizedText'][0])\n",
        "\n",
        "        print(\"Tokenizing...\")\n",
        "        \n",
        "        # Tokenize the text data\n",
        "        df['tokenized'] = df['text'].str.lower().apply(nltk.word_tokenize)\n",
        "\n",
        "        #return df, df['category'].unique()\n",
        "        return df\n",
        "    \n",
        "    elif (dataset == 'bbc-news'):\n",
        "\n",
        "        print(\"Loading BBC News dataset...\")\n",
        "\n",
        "        for dirname, _, filenames in os.walk(DATASET_DIR+'bbc-news'):\n",
        "            for filename in filenames:\n",
        "                print(os.path.join(dirname, filename))\n",
        "\n",
        "        train_set = pd.read_csv(DATASET_DIR+'bbc-news/BBC News Train.csv')\n",
        "        test_set = pd.read_csv(DATASET_DIR+'bbc-news/BBC News Test.csv')\n",
        "\n",
        "        print(\"train_set:\", train_set.shape)\n",
        "        print(train_set.head())\n",
        "        print(\"test_set:\", test_set.shape)\n",
        "        print(test_set.head())\n",
        "\n",
        "        target_category = train_set['Category'].unique()\n",
        "        print(\"target categories:\", target_category)\n",
        "\n",
        "        train_set['categoryId'] = train_set['Category'].factorize()[0]\n",
        "        \n",
        "        category = train_set[[\"Category\",\"categoryId\"]].drop_duplicates().sort_values('categoryId')\n",
        "        print(\"after de-duping:\", category)\n",
        "\n",
        "        print(train_set.groupby('Category').categoryId.count())\n",
        "\n",
        "        text = train_set[\"Text\"] \n",
        "        print(\"text:\\n\", text.head())\n",
        "\n",
        "        category = train_set[\"Category\"]\n",
        "        print(\"categories:\\n\", category.head())\n",
        "\n",
        "        print(\"preprocessing...\")\n",
        "        train_set['Text'] = train_set['Text'].apply(preprocessDataset)\n",
        "        text = train_set['Text']\n",
        "        category = train_set['Category']\n",
        "        print(\"text:\\n\", type(text), text.shape)\n",
        "        print(text.head())\n",
        "\n",
        "        #return text, category\n",
        "        return train_set\n",
        "    else:\n",
        "        print(f\"Dataset '{dataset}' not available.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "# Core processing function\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "def classify(dataset='20newsgrouops', args=None):\n",
        "    \n",
        "    if (args is None):\n",
        "        print(\"No arguments passed.\")\n",
        "        return\n",
        "    \n",
        "    if (args.dataset not in dataset_available):\n",
        "        print(f\"Dataset '{args.dataset}' not available.\")\n",
        "        return\n",
        "    \n",
        "    pickle_file_name=f'{dataset}_{args.mode}_tokenized.pickle'\n",
        "\n",
        "    print(f\"Classifying {dataset}...\")\n",
        "\n",
        "    print(f\"Loading data set {dataset}...\")\n",
        "\n",
        "    # Define the path to the pickle file\n",
        "    pickle_file = PICKLE_DIR + pickle_file_name\n",
        "\n",
        "    if os.path.exists(pickle_file):                                         # if the pickle file exists\n",
        "        \n",
        "        print(f\"Loading tokenized data from '{pickle_file}'...\")\n",
        "        \n",
        "        if (dataset == '20newsgroups'):\n",
        "            # Initialize an empty DataFrame with the desired columns\n",
        "            columns = ['tokenized', 'CleanedText', 'LemmatizedText', 'category', 'category_name', 'text']\n",
        "            \n",
        "            df = pd.DataFrame(columns=columns)\n",
        "            \n",
        "            # Load the data from the pickle file into the DataFrame\n",
        "            with open(pickle_file, 'rb') as f:\n",
        "                df = pickle.load(f)\n",
        "\n",
        "            #categories = df['category'].unique()\n",
        "            \n",
        "        elif (dataset == 'bbc-news'):\n",
        "\n",
        "            columns = []\n",
        "\n",
        "            df = pd.DataFrame(columns=columns)\n",
        "            \n",
        "            with open(pickle_file, 'rb') as f:\n",
        "                df = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"'{pickle_file}' not found, retrieving and preprocessing data set {dataset}...\")\n",
        "\n",
        "        #df, categories = load_data(dataset)\n",
        "        df = load_data(dataset)\n",
        "\n",
        "        print(\"df:\", df.shape)\n",
        "        print(df.head())\n",
        "\n",
        "        # Save the tokenized DataFrame to a pickle file\n",
        "        if (dataset == '20newsgroups'):\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(df[['tokenized', 'CleanedText', 'LemmatizedText', 'category', 'category_name', 'text']], f)\n",
        "\n",
        "        elif (dataset == 'bbc-news'):\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(df[['ArticleId', 'Text', 'Category']], f)\n",
        "                #pickle.dump(df, f)\n",
        "\n",
        "    print(\"Tokenized data loaded, df:\", df.shape)\n",
        "    print(df.head())\n",
        "    #print(\"categories:\", categories)\n",
        "\n",
        "    print(\"Processing data...\")\n",
        "\n",
        "    if (dataset == '20newsgroups'):\n",
        "\n",
        "        print(\"classifying 20 newsgroups...\")\n",
        "\n",
        "        \"\"\"\n",
        "        # POS Tagging and Counting\n",
        "        tagged_titles = df['text'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
        "\n",
        "        def count_tags(title_with_tags):\n",
        "            tag_count = {}\n",
        "            for word, tag in title_with_tags:\n",
        "                tag_count[tag] = tag_count.get(tag, 0) + 1\n",
        "            return tag_count\n",
        "\n",
        "        # Create a DataFrame with POS tag counts\n",
        "        tagged_titles_df = pd.DataFrame(tagged_titles.apply(lambda x: count_tags(x)).tolist()).fillna(0)\n",
        "\n",
        "        # Sum the occurrences of each tag across all documents\n",
        "        tagged_titles_sum = tagged_titles_df.sum().sort_values(ascending=False)\n",
        "\n",
        "        # Plot POS Tag Frequency\n",
        "        trace = go.Bar(x=tagged_titles_sum.index, y=tagged_titles_sum.values)\n",
        "        layout = go.Layout(title='Frequency of POS Tags in IT Support Tickets Dataset', xaxis=dict(title='POS'), yaxis=dict(title='Count'))\n",
        "        fig = go.Figure(data=[trace], layout=layout)\n",
        "\n",
        "        # This will open the plot in the default web browser\n",
        "        pyo.plot(fig, filename='../../out/pos_tag_frequency.html')\n",
        "        \"\"\"\n",
        "\n",
        "        # Feature Extraction and Model Training\n",
        "        print(\"Splitting the dataset...\")\n",
        "\n",
        "        X = df['CleanedText']\n",
        "        y = df['category']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44)\n",
        "\n",
        "        print(\"X_train:\", type(X_train), X_train.shape)\n",
        "        print(\"X_test:\", type(X_test), X_test.shape)\n",
        "        \n",
        "        print(\"Y_train:\", type(y_train), y_train.shape)\n",
        "        print(\"Y_test:\", type(y_test), y_test.shape)\n",
        "\n",
        "        print(\"Running model...\")\n",
        "\n",
        "        run_model(X_train, X_test, y_train, y_test, args)\n",
        "    \n",
        "    elif (dataset == 'bbc-news'):\n",
        "\n",
        "        print(f'classifying bbc-news...')\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['Text'],\n",
        "            df['Category'], \n",
        "            test_size = 0.3, \n",
        "            random_state = 60,\n",
        "            shuffle=True, \n",
        "            stratify=df['Category']\n",
        "            )\n",
        "\n",
        "        print(\"X_train:\", type(X_train), X_train.shape)\n",
        "        print(\"X_test:\", type(X_test), X_test.shape)\n",
        "        \n",
        "        print(\"Y_train:\", type(y_train), y_train.shape)\n",
        "        print(\"Y_test:\", type(y_test), y_test.shape)\n",
        "\n",
        "        print(\"Running model...\")\n",
        "\n",
        "        run_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_svm_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    print(\"Training default Support Vector Machine model...\")\n",
        "    \n",
        "    default_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('lr', LinearSVC(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    default_pipeline.fit(X_train, y_train)\n",
        "    y_pred_default = default_pipeline.predict(X_test)\n",
        "\n",
        "    print(\"\\nDefault Support Vector Mechine Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_default):.4f}\")\n",
        "    print(classification_report(y_true=y_test, y_pred=y_pred_default, digits=4))\n",
        "\n",
        "    if (args.optimc):\n",
        "\n",
        "        # Optimize Support Vector Machine with GridSearchCV\n",
        "        print(\"Optimizing Support Vector Machine model with GridSearchCV...\")\n",
        "\n",
        "        # Define the pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('svm', LinearSVC(max_iter=1000))\n",
        "        ])\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],     # Unigrams, bigrams, or trigrams\n",
        "            'tfidf__use_idf': [True, False],                    # Whether to use IDF\n",
        "            'tfidf__sublinear_tf': [True, False],               # Sublinear term frequency\n",
        "            'svm__penalty': ['l1', 'l2'],                       # Regularization method\n",
        "            'svm__loss': ['hinge', 'squared_hinge'],            # Loss function\n",
        "            'svm__multi_class': ['ovr', 'crammer_singer'],      # Multi-class strategy\n",
        "            'svm__class_weight': [None, 'balanced'],            # Class weights\n",
        "            'svm__C': np.logspace(-3, 3, 7)                     # Regularization parameter   \n",
        "        }\n",
        "\n",
        "        print(\"param_grid:\", param_grid)\n",
        "\n",
        "        cross_validation = StratifiedKFold()\n",
        "\n",
        "        scorers = {\n",
        "            'accuracy_score': make_scorer(accuracy_score),\n",
        "            'f1_score': make_scorer(f1_score, average='micro'),\n",
        "            'recall_score': make_scorer(recall_score, average='micro'),\n",
        "            'precision_score': make_scorer(precision_score, average='micro'),\n",
        "            'hamming_loss': make_scorer(hamming_loss),\n",
        "            'jaccard_score': make_scorer(jaccard_score, average='micro')\n",
        "            }\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            n_jobs=-1, \n",
        "            estimator=pipeline,\n",
        "            refit='f1_score',\n",
        "            param_grid=param_grid,\n",
        "            cv=cross_validation,\n",
        "            #scoring=scoring\n",
        "            scoring=scorers,\n",
        "            return_train_score=True         # ensure train scores are calculated\n",
        "            )\n",
        "\n",
        "        # Fit the model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "        print(\"best_estimator:\", grid_search.best_estimator_)\n",
        "        print('Best score: {}'.format(grid_search.best_score_))\n",
        "        print(\"cv_results_:\", grid_search.cv_results_)\n",
        "\n",
        "        results = grid_search.cv_results_\n",
        "\n",
        "        if (args.plot):\n",
        "\n",
        "            print(\"Plotting the results...\")\n",
        "\n",
        "            # Define the metrics we want to plot\n",
        "            metrics_to_plot = ['accuracy_score', 'f1_score', 'recall_score', 'precision_score', 'hamming_loss']\n",
        "\n",
        "            # Iterate over each metric to create a separate plot\n",
        "            for metric in metrics_to_plot:\n",
        "                traces = []\n",
        "\n",
        "                print(f\"Plotting {metric}...\")\n",
        "\n",
        "                for sample in [\"train\", \"test\"]:\n",
        "\n",
        "                    key_mean = f\"mean_{sample}_{metric}\"\n",
        "                    key_std = f\"std_{sample}_{metric}\"\n",
        "\n",
        "                    print(f\"Plotting {key_mean}...\")\n",
        "                    print(f\"Plotting {key_std}...\")\n",
        "\n",
        "                    # Directly use the keys without conditional check\n",
        "                    sample_score_mean = np.nan_to_num(np.array(results[key_mean]) * 100)  # Convert to percentage and handle NaN\n",
        "                    sample_score_std = np.nan_to_num(np.array(results[key_std]) * 100)  # Convert to percentage and handle NaN\n",
        "\n",
        "                    x_axis = np.linspace(0, 100, len(sample_score_mean))\n",
        "\n",
        "                    # Create the trace for Plotly\n",
        "                    traces.append(\n",
        "                        go.Scatter(\n",
        "                            x=x_axis,\n",
        "                            y=sample_score_mean,\n",
        "                            mode='lines+markers',\n",
        "                            name=f\"{metric} ({sample})\",\n",
        "                            line=dict(dash='dash' if sample == 'train' else 'solid'),\n",
        "                            error_y=dict(\n",
        "                                type='data',\n",
        "                                array=sample_score_std,\n",
        "                                visible=True\n",
        "                            ),\n",
        "                            hoverinfo='x+y+name'\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Define the layout of the plot\n",
        "                layout = go.Layout(\n",
        "                    title={'text': f\"Training and Test Scores for {metric.capitalize()}\",\n",
        "                        'y':0.9,\n",
        "                        'x':0.5,\n",
        "                        'xanchor': 'center',\n",
        "                        'yanchor': 'top'},\n",
        "                    xaxis=dict(title=\"Training Sample Percentage (%)\"),\n",
        "                    yaxis=dict(title=\"Score (%)\", range=[0, 100]),\n",
        "                    hovermode='closest'\n",
        "                )\n",
        "\n",
        "                # Create the figure\n",
        "                fig = go.Figure(data=traces, layout=layout)\n",
        "\n",
        "                # Write the plot to an HTML file\n",
        "                filename = f'{OUT_DIR}training_test_scores_{metric}.html'\n",
        "                pyo.plot(fig, filename=filename)\n",
        "\n",
        "                print(f\"Saved plot for {metric} as {filename}\")\n",
        "\n",
        "        # Extract the best estimator from the GridSearchCV\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # Predict on the test set using the best model\n",
        "        y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "        print(\"Accuracy best score:\", metrics.accuracy_score(y_test, y_pred_best))\n",
        "        print(classification_report(y_true=y_test, y_pred=y_pred_best, digits=4))\n",
        "\n",
        "\n",
        "def run_lr_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    # Default Logistic Regression Model\n",
        "    print(\"Training default Logistic Regression model...\")\n",
        "    default_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('lr', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    default_pipeline.fit(X_train, y_train)\n",
        "    y_pred_default = default_pipeline.predict(X_test)\n",
        "\n",
        "    print(\"\\nDefault Logistic Regression Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_default):.4f}\")\n",
        "    print(classification_report(y_true=y_test, y_pred=y_pred_default, digits=4))\n",
        "\n",
        "    if (args.optimc):\n",
        "        # Optimize Logistic Regression with GridSearchCV\n",
        "        print(\"Optimizing Logistic Regression model with GridSearchCV...\")\n",
        "\n",
        "        # Define the pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('lr', LogisticRegression(max_iter=1000))\n",
        "        ])\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],     # Unigrams, bigrams, or trigrams\n",
        "            'tfidf__use_idf': [True, False],                    # Whether to use IDF\n",
        "            'tfidf__sublinear_tf': [True, False],               # Sublinear term frequency\n",
        "            'lr__C': [0.01, 0.1, 1, 10, 100],                   # Inverse of regularization strength\n",
        "            'lr__penalty': ['l2'],                              # Regularization method (L2 Ridge)\n",
        "            'lr__solver': ['liblinear', 'lbfgs']                # Solver types\n",
        "        }\n",
        "\n",
        "        print(\"param_grid:\", param_grid)\n",
        "\n",
        "        # Define scorers\n",
        "        scorers = {\n",
        "            'accuracy_score': make_scorer(accuracy_score),\n",
        "            'f1_score': make_scorer(f1_score, average='micro'),\n",
        "            'recall_score': make_scorer(recall_score, average='micro'),\n",
        "            'precision_score': make_scorer(precision_score, average='micro')\n",
        "        }\n",
        "\n",
        "        # Initialize GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=scorers,\n",
        "            refit='f1_score',  # Optimize on F1 Score\n",
        "            cv=StratifiedKFold(n_splits=5),\n",
        "            n_jobs=-1,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Display the best parameters\n",
        "        print('Best parameters found by GridSearchCV:')\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        y_pred_optimized = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "        print(\"\\nOptimized Logistic Regression Model Performance:\")\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred_optimized):.4f}\")\n",
        "        print(classification_report(y_true=y_test, y_pred=y_pred_optimized, digits=4))\n",
        "\n",
        "    if (args.cm):\n",
        "        # Optionally, plot confusion matrix for the optimized model\n",
        "        create_confusion_matrix(\n",
        "            y_test, \n",
        "            y_pred_optimized, \n",
        "            title='Confusion Matrix for Optimized Logistic Regression Model',\n",
        "            file_name=OUT_DIR+'bbc_news_logistic_regression_confusion_matrix.png',\n",
        "            debug=False\n",
        "        )\n",
        "\n",
        "\n",
        "def run_nb_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    print(\"Building default Naive Bayes Classifier...\")\n",
        "\n",
        "    nb = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', MultinomialNB())\n",
        "        ])\n",
        "    \n",
        "    nb.fit(X_train,y_train)\n",
        "\n",
        "    test_predict = nb.predict(X_test)\n",
        "\n",
        "    train_accuracy = round(nb.score(X_train,y_train)*100)\n",
        "    test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
        "\n",
        "    print(\"Naive Bayes Train Accuracy Score : {}% \".format(train_accuracy ))\n",
        "    print(\"Naive Bayes Test Accuracy Score  : {}% \".format(test_accuracy ))\n",
        "    print(classification_report(y_true=test_predict, y_pred=y_test, digits=4))\n",
        "\n",
        "    if (args.optimc):\n",
        "\n",
        "        print(\"Optimizing the model using GridSearchCV...\")\n",
        "\n",
        "        # Define a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('nb', MultinomialNB())\n",
        "        ])\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],         # Unigrams, bigrams, or trigrams\n",
        "            'tfidf__use_idf': [True, False],                        # Whether to use IDF\n",
        "            'tfidf__sublinear_tf': [True, False],                   # Sublinear term frequency\n",
        "            'nb__alpha': [0.1, 0.5, 1.0, 1.5, 2.0],                 # Smoothing parameter for Naive Bayes\n",
        "        }\n",
        "\n",
        "        print(\"param_grid:\", param_grid)\n",
        "\n",
        "        # Define scorers\n",
        "        scorers = {\n",
        "            'accuracy_score': make_scorer(accuracy_score),\n",
        "            'f1_score': make_scorer(f1_score, average='micro'),\n",
        "            'recall_score': make_scorer(recall_score, average='micro'),\n",
        "            'precision_score': make_scorer(precision_score, average='micro'),\n",
        "            'hamming_loss': make_scorer(hamming_loss),\n",
        "            'jaccard_score': make_scorer(jaccard_score, average='micro')\n",
        "        }\n",
        "\n",
        "        # Initialize GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=scorers,\n",
        "            refit='f1_score',                           # Optimize on F1 Score\n",
        "            cv=StratifiedKFold(n_splits=5),\n",
        "            n_jobs=-1,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Display the best parameters\n",
        "        print('Best parameters found by GridSearchCV:')\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "        print(\"\\nBest Estimator's Test Set Performance:\")\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(f\"F1 Score: {f1_score(y_test, y_pred, average='micro'):.4f}\")\n",
        "        print(f\"Recall: {recall_score(y_test, y_pred, average='micro'):.4f}\")\n",
        "        print(f\"Precision: {precision_score(y_test, y_pred, average='micro'):.4f}\")\n",
        "        print(classification_report(y_true=y_test, y_pred=y_pred, digits=4))\n",
        "\n",
        "        if (args.cm):\n",
        "            # Optionally, plot confusion matrix\n",
        "            create_confusion_matrix(\n",
        "                y_test, \n",
        "                y_pred, \n",
        "                title='Confusion Matrix for Optimized Naive Bayes Model',\n",
        "                file_name=OUT_DIR+'bbc_news_naive_bayes_confusion_matrix.png',\n",
        "                debug=False\n",
        "            )\n",
        "\n",
        "\n",
        "def run_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    print(\"Running model...\")\n",
        "\n",
        "    # Support Vector Machine Classifier\n",
        "    if (args.learner == 'svm'):\n",
        "        run_svm_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "    # Logistic Regression Classifier\n",
        "    elif (args.learner == 'lr'):\n",
        "        run_lr_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "    # Naive Bayes (MultinomialNB) Classifier\n",
        "    elif (args.learner == 'nb'):\n",
        "        run_nb_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "    elif (args.learner == 'dt'):\n",
        "        print(\"Decision Tree Classifier\")\n",
        "        dt = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('dt', DecisionTreeClassifier())\n",
        "            ])\n",
        "\n",
        "        dt.fit(X_train, y_train)\n",
        "\n",
        "        test_predict = dt.predict(X_test)\n",
        "\n",
        "        train_accuracy = round(dt.score(X_train, y_train)*100)\n",
        "        test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
        "\n",
        "        print(\"Decision Tree Train Accuracy Score : {}% \".format(train_accuracy ))\n",
        "        print(\"Decision Tree Test Accuracy Score  : {}% \".format(test_accuracy ))\n",
        "        print(classification_report(y_true=test_predict, y_pred=y_test, digits=4))\n",
        "\n",
        "    elif (args.learner == 'rf'):\n",
        "\n",
        "        print(\"Random Forest Classifier\")\n",
        "        rfc = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('rfc', RandomForestClassifier(n_estimators=100))\n",
        "            ])\n",
        "\n",
        "        rfc.fit(X_train, y_train)\n",
        "\n",
        "        test_predict = rfc.predict(X_test)\n",
        "\n",
        "        train_accuracy = round(rfc.score(X_train, y_train)*100)\n",
        "        test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
        "\n",
        "        print(\"K-Nearest Neighbour Train Accuracy Score : {}% \".format(train_accuracy ))\n",
        "        print(\"K-Nearest Neighbour Test Accuracy Score  : {}% \".format(test_accuracy ))\n",
        "        print(classification_report(y_true=test_predict, y_pred=y_test, digits=4))\n",
        "\n",
        "    else:\n",
        "        print(f\"Invalid learner '{args.learner}'\")\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data set bbc-news...\n",
            "Loading BBC News dataset...\n",
            "../../datasets/bbc-news/BBC News Train.csv\n",
            "../../datasets/bbc-news/.DS_Store\n",
            "../../datasets/bbc-news/BBC News Test.csv\n",
            "../../datasets/bbc-news/BBC News Sample Solution.csv\n",
            "train_set: (1490, 3)\n",
            "   ArticleId                                               Text  Category\n",
            "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
            "1        154  german business confidence slides german busin...  business\n",
            "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
            "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
            "4        917  enron bosses in $168m payout eighteen former e...  business\n",
            "test_set: (735, 2)\n",
            "   ArticleId                                               Text\n",
            "0       1018  qpr keeper day heads for preston queens park r...\n",
            "1       1319  software watching while you work software that...\n",
            "2       1138  d arcy injury adds to ireland woe gordon d arc...\n",
            "3        459  india s reliance family feud heats up the ongo...\n",
            "4       1020  boro suffer morrison injury blow middlesbrough...\n",
            "target categories: ['business' 'tech' 'politics' 'sport' 'entertainment']\n",
            "after de-duping:         Category  categoryId\n",
            "0       business           0\n",
            "3           tech           1\n",
            "5       politics           2\n",
            "6          sport           3\n",
            "7  entertainment           4\n",
            "Category\n",
            "business         336\n",
            "entertainment    273\n",
            "politics         274\n",
            "sport            346\n",
            "tech             261\n",
            "Name: categoryId, dtype: int64\n",
            "text:\n",
            " 0    worldcom ex-boss launches defence lawyers defe...\n",
            "1    german business confidence slides german busin...\n",
            "2    bbc poll indicates economic gloom citizens in ...\n",
            "3    lifestyle  governs mobile choice  faster  bett...\n",
            "4    enron bosses in $168m payout eighteen former e...\n",
            "Name: Text, dtype: object\n",
            "categories:\n",
            " 0    business\n",
            "1    business\n",
            "2    business\n",
            "3        tech\n",
            "4    business\n",
            "Name: Category, dtype: object\n",
            "preprocessing...\n",
            "text:\n",
            " <class 'pandas.core.series.Series'> (1490,)\n",
            "0    worldcom ex-boss launch defenc lawyer defend f...\n",
            "1    german busi confid slide german busi confid fe...\n",
            "2    bbc poll indic econom gloom citizen major nati...\n",
            "3    lifestyl govern mobil choic faster well funkie...\n",
            "4    enron bos $ m payout eighteen former enron dir...\n",
            "Name: Text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "data = load_data('bbc-news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J7pxO51TztH",
        "outputId": "25946114-415a-47d1-c5ec-23072a279bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LxH5sC6AdrwzClPEGUAV2HVcPjWIPmc8\n",
            "To: /content/FinalFinalTwoTierSnowmirror.csv\n",
            "100% 101M/101M [00:00<00:00, 217MB/s] \n"
          ]
        }
      ],
      "source": [
        "! gdown --id 1LxH5sC6AdrwzClPEGUAV2HVcPjWIPmc8  ##SupportTickets\n",
        "#! gdown --id 1etLBrBTdokVHIuaxEmr1Koacos4IbMZL ## ConsumerComplaints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Et3A9nrTzvT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv('FinalFinalTwoTierSnowmirror.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "QswhfKYMT5V2",
        "outputId": "ebb07af4-aeb9-4e0e-a200-8dacc5b52c54"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FinalText</th>\n",
              "      <th>DV_CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mass data migration device rental invoice</td>\n",
              "      <td>Project Office</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ibm cloud backup non successful hello use evau...</td>\n",
              "      <td>Compute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>accounting request pro rate credit</td>\n",
              "      <td>Project Office</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>feature code purchase subscription receive sen...</td>\n",
              "      <td>Project Office</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>subscription account link ibm cloud bluemix si...</td>\n",
              "      <td>Project Office</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>virtual server high customer like replicate ib...</td>\n",
              "      <td>Compute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>incorrectly early provision due error replace ...</td>\n",
              "      <td>Project Office</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>security compliance require vmware answer impl...</td>\n",
              "      <td>Compute</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>service cancellation</td>\n",
              "      <td>3rd Party Reseller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bluemix asset monitor compose postgresql</td>\n",
              "      <td>Platform / Console</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           FinalText         DV_CATEGORY\n",
              "0          mass data migration device rental invoice      Project Office\n",
              "1  ibm cloud backup non successful hello use evau...             Compute\n",
              "2                 accounting request pro rate credit      Project Office\n",
              "3  feature code purchase subscription receive sen...      Project Office\n",
              "4  subscription account link ibm cloud bluemix si...      Project Office\n",
              "5  virtual server high customer like replicate ib...             Compute\n",
              "6  incorrectly early provision due error replace ...      Project Office\n",
              "7  security compliance require vmware answer impl...             Compute\n",
              "8                               service cancellation  3rd Party Reseller\n",
              "9           bluemix asset monitor compose postgresql  Platform / Console"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##  Creating a new Dataframe with the cols we're interested In\n",
        "cols = ['FinalText', 'DV_CATEGORY']\n",
        "#cols = ['TokenizedText', 'Product']\n",
        "data = data[cols]\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq9flKMpBxKz",
        "outputId": "5dbb2dce-715c-42e6-d6f5-6a6da3081ff8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Project Office', 'Compute', '3rd Party Reseller',\n",
              "       'Platform / Console', 'Services', 'VPC', 'Apps',\n",
              "       'Security and Identity', 'Networking', 'Integration',\n",
              "       'Project Office (internal)', 'Storage'], dtype=object)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.DV_CATEGORY.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c5YVYnTT5YX"
      },
      "outputs": [],
      "source": [
        "X_data = data[['FinalText']].to_numpy().reshape(-1)\n",
        "y_data = data[['DV_CATEGORY']].to_numpy().reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjD9-_y-T5ab"
      },
      "outputs": [],
      "source": [
        "def bert_encode(texts, tokenizer):\n",
        "    ct = len(texts)\n",
        "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
        "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
        "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
        "\n",
        "    for k, text in enumerate(texts):\n",
        "        # Tokenize\n",
        "        tok_text = tokenizer.tokenize(str(text))\n",
        "\n",
        "        # Truncate and convert tokens to numerical IDs\n",
        "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
        "\n",
        "        input_length = len(enc_text) + 2\n",
        "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
        "\n",
        "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
        "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
        "\n",
        "        # Set to 1s in the attention input\n",
        "        attention_mask[k,:input_length] = 1\n",
        "\n",
        "    return {\n",
        "        'input_word_ids': input_ids,\n",
        "        'input_mask': attention_mask,\n",
        "        'input_type_ids': token_type_ids\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qdNSkLUT5cV",
        "outputId": "175a556f-7c8f-4841-ac99-2960bf41f810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'Project Office',\n",
              " 1: 'Compute',\n",
              " 2: '3rd Party Reseller',\n",
              " 3: 'Platform / Console',\n",
              " 4: 'Services',\n",
              " 5: 'VPC',\n",
              " 6: 'Apps',\n",
              " 7: 'Security and Identity',\n",
              " 8: 'Networking',\n",
              " 9: 'Integration',\n",
              " 10: 'Project Office (internal)',\n",
              " 11: 'Storage'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform categories into numbers\n",
        "category_to_id = {}\n",
        "category_to_name = {}\n",
        "\n",
        "for index, c in enumerate(y_data):\n",
        "    if c in category_to_id:\n",
        "        category_id = category_to_id[c]\n",
        "    else:\n",
        "        category_id = len(category_to_id)\n",
        "        category_to_id[c] = category_id\n",
        "        category_to_name[category_id] = c\n",
        "\n",
        "    y_data[index] = category_id\n",
        "\n",
        "# Display dictionary\n",
        "category_to_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HOoBzcqT5d8"
      },
      "outputs": [],
      "source": [
        "categories = data['DV_CATEGORY'].unique()\n",
        "n_categories = len(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Q2f0HPI7kN",
        "outputId": "b0b063e0-b528-44d1-ea12-867c36e58f76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNcQ3GWVT5f-"
      },
      "outputs": [],
      "source": [
        "# Split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=777) # random_state to reproduce results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUeQhYKxIMMA",
        "outputId": "fbc8105c-dc0c-45d6-b51b-ab871f0c1c0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['increase upgrade bandwidth option sale agent hello need set server account expect consumption last month local ibm',\n",
              "       'make lite account get confirmation mail hour address trial ibm must spam box think proceed',\n",
              "       'user provide create dev environment access name hasan mohammad regard organization space resource field',\n",
              "       ..., 'test ticket ignore',\n",
              "       'confirmation behavior auto scale hello support team use node red like ask question automatically start instance number alive become low minimal least confirm scaling answer yes long take since',\n",
              "       'request survey charge san provide usage report something check content renewal order total balance platform support month reflect amount overage investigate whether follow invoice correct'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9YDnMKjT5iJ"
      },
      "outputs": [],
      "source": [
        "# Import tokenizer from HuggingFace\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7RLCMqT5kA"
      },
      "outputs": [],
      "source": [
        "X_train = bert_encode(X_train, tokenizer)\n",
        "X_test = bert_encode(X_test, tokenizer)\n",
        "\n",
        "y_train = np.asarray(y_train, dtype='int32')\n",
        "y_test = np.asarray(y_test, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOvA9Ns7UZIY"
      },
      "outputs": [],
      "source": [
        "def build_model(n_categories):\n",
        "    with strategy.scope():\n",
        "        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
        "        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
        "\n",
        "        # Import BERT model from HuggingFace\n",
        "        bert_model = TFBertModel.from_pretrained(MODEL_NAME)\n",
        "        x = bert_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
        "\n",
        "        # Huggingface transformers have multiple outputs, embeddings are the first one,\n",
        "        # so let's slice out the first position\n",
        "        x = x[0]\n",
        "\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stSAgjOpUZLK",
        "outputId": "e70a4242-05de-4728-f89f-bd2d19580e30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_3 (TFBertModel)   TFBaseModelOutputWit 109482240   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 256, 768)     0           tf_bert_model_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 196608)       0           dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          50331904    flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 12)           3084        dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 159,817,228\n",
            "Trainable params: 159,817,228\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model(n_categories)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJtUEjSvUZOi",
        "outputId": "767b4a86-5023-4ff4-d358-fce3fd474684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None,) dtype=int32>]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None,) dtype=int32>]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2128/2128 [==============================] - ETA: 0s - loss: 0.8341 - accuracy: 0.7366"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 256) dtype=int32>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None,) dtype=int32>]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2128/2128 [==============================] - 508s 199ms/step - loss: 0.8341 - accuracy: 0.7366 - val_loss: 0.7145 - val_accuracy: 0.7733\n",
            "Epoch 2/3\n",
            "2128/2128 [==============================] - 355s 167ms/step - loss: 0.6804 - accuracy: 0.7811 - val_loss: 0.6705 - val_accuracy: 0.7896\n",
            "Epoch 3/3\n",
            "2128/2128 [==============================] - 356s 167ms/step - loss: 0.6180 - accuracy: 0.7988 - val_loss: 0.6590 - val_accuracy: 0.7917\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    print('Training...')\n",
        "    history = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs=EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "130Vi7HuKj22",
        "outputId": "e53deb68-38ec-4168-8dc0-0522d322f66b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.9132952 , 0.08670478],\n",
              "       [0.09615043, 0.90384954],\n",
              "       [0.90086865, 0.09913131],\n",
              "       [0.91933906, 0.08066092],\n",
              "       [0.12343215, 0.87656784],\n",
              "       [0.8872177 , 0.11278225],\n",
              "       [0.16934107, 0.830659  ],\n",
              "       [0.90292263, 0.09707733],\n",
              "       [0.04330614, 0.9566938 ],\n",
              "       [0.9645212 , 0.03547875],\n",
              "       [0.95503354, 0.04496641],\n",
              "       [0.8570329 , 0.14296713],\n",
              "       [0.06058738, 0.9394126 ],\n",
              "       [0.8674663 , 0.13253374],\n",
              "       [0.27896786, 0.72103214],\n",
              "       [0.09123947, 0.9087604 ],\n",
              "       [0.92922574, 0.07077418],\n",
              "       [0.8877487 , 0.11225132],\n",
              "       [0.26868767, 0.73131233],\n",
              "       [0.06149247, 0.93850744],\n",
              "       [0.85076886, 0.14923121],\n",
              "       [0.2576749 , 0.7423251 ],\n",
              "       [0.03756809, 0.96243197],\n",
              "       [0.76388615, 0.23611389],\n",
              "       [0.03847211, 0.9615279 ],\n",
              "       [0.9822324 , 0.01776751],\n",
              "       [0.04295966, 0.95704037],\n",
              "       [0.83125776, 0.1687422 ],\n",
              "       [0.88538367, 0.11461624],\n",
              "       [0.8722662 , 0.12773384],\n",
              "       [0.9014038 , 0.09859616],\n",
              "       [0.9863839 , 0.0136161 ],\n",
              "       [0.19559804, 0.804402  ],\n",
              "       [0.45445403, 0.54554605],\n",
              "       [0.84989035, 0.15010971],\n",
              "       [0.8829321 , 0.1170679 ],\n",
              "       [0.09255505, 0.90744495],\n",
              "       [0.1992919 , 0.80070806],\n",
              "       [0.88333607, 0.11666393],\n",
              "       [0.7388362 , 0.2611637 ],\n",
              "       [0.9390301 , 0.06096981],\n",
              "       [0.791823  , 0.20817697],\n",
              "       [0.9486282 , 0.05137178],\n",
              "       [0.03567509, 0.96432483],\n",
              "       [0.05251979, 0.9474802 ],\n",
              "       [0.91659296, 0.08340711],\n",
              "       [0.78296906, 0.217031  ],\n",
              "       [0.90973264, 0.0902674 ],\n",
              "       [0.03393042, 0.9660695 ],\n",
              "       [0.17323503, 0.826765  ],\n",
              "       [0.81460696, 0.18539305],\n",
              "       [0.83794147, 0.16205847],\n",
              "       [0.1353766 , 0.86462337],\n",
              "       [0.03530122, 0.96469885],\n",
              "       [0.9419799 , 0.05802007],\n",
              "       [0.6568753 , 0.34312466],\n",
              "       [0.9556335 , 0.04436655],\n",
              "       [0.93144554, 0.06855443],\n",
              "       [0.9216193 , 0.07838079],\n",
              "       [0.03847211, 0.9615279 ],\n",
              "       [0.9440521 , 0.05594793],\n",
              "       [0.9190868 , 0.08091328],\n",
              "       [0.9085092 , 0.09149077],\n",
              "       [0.0760777 , 0.92392224],\n",
              "       [0.31922534, 0.6807747 ],\n",
              "       [0.23708406, 0.7629159 ],\n",
              "       [0.20601688, 0.79398316],\n",
              "       [0.8377378 , 0.16226214],\n",
              "       [0.8831733 , 0.11682679],\n",
              "       [0.03530122, 0.96469885],\n",
              "       [0.18249051, 0.8175095 ],\n",
              "       [0.93751115, 0.0624888 ],\n",
              "       [0.12652993, 0.87347007],\n",
              "       [0.7567282 , 0.24327184],\n",
              "       [0.05527294, 0.9447271 ],\n",
              "       [0.90804136, 0.09195859],\n",
              "       [0.9348401 , 0.0651599 ],\n",
              "       [0.04492054, 0.9550795 ],\n",
              "       [0.8795277 , 0.12047236],\n",
              "       [0.11449827, 0.88550174],\n",
              "       [0.2487114 , 0.7512886 ],\n",
              "       [0.77219963, 0.22780035],\n",
              "       [0.7534887 , 0.24651128],\n",
              "       [0.924446  , 0.07555399],\n",
              "       [0.908393  , 0.09160704],\n",
              "       [0.8885408 , 0.11145917],\n",
              "       [0.07970972, 0.9202903 ],\n",
              "       [0.20220083, 0.79779917],\n",
              "       [0.10497375, 0.8950262 ],\n",
              "       [0.38690266, 0.61309725],\n",
              "       [0.1599816 , 0.84001845],\n",
              "       [0.89193463, 0.10806547],\n",
              "       [0.17124577, 0.82875425],\n",
              "       [0.91379917, 0.08620086],\n",
              "       [0.8289385 , 0.17106152],\n",
              "       [0.9402648 , 0.05973525],\n",
              "       [0.03677799, 0.963222  ],\n",
              "       [0.90076274, 0.09923722],\n",
              "       [0.15749994, 0.8425001 ],\n",
              "       [0.9864181 , 0.0135819 ],\n",
              "       [0.04363226, 0.9563677 ],\n",
              "       [0.9006993 , 0.09930068],\n",
              "       [0.9366036 , 0.06339634],\n",
              "       [0.9024023 , 0.09759776],\n",
              "       [0.10188842, 0.8981116 ],\n",
              "       [0.87506497, 0.12493496],\n",
              "       [0.9280425 , 0.0719576 ],\n",
              "       [0.93428963, 0.06571034],\n",
              "       [0.9170511 , 0.08294891],\n",
              "       [0.908586  , 0.09141394],\n",
              "       [0.91645676, 0.08354331],\n",
              "       [0.22758299, 0.772417  ],\n",
              "       [0.02572891, 0.9742712 ],\n",
              "       [0.17001097, 0.829989  ],\n",
              "       [0.04089729, 0.95910275],\n",
              "       [0.905689  , 0.09431106],\n",
              "       [0.87811464, 0.12188534],\n",
              "       [0.10274567, 0.8972543 ],\n",
              "       [0.9051146 , 0.09488548],\n",
              "       [0.18485405, 0.8151459 ],\n",
              "       [0.22707817, 0.77292186],\n",
              "       [0.91797   , 0.08203001],\n",
              "       [0.02640554, 0.9735945 ],\n",
              "       [0.9372347 , 0.06276528],\n",
              "       [0.8995333 , 0.10046674],\n",
              "       [0.09186717, 0.9081328 ],\n",
              "       [0.9321659 , 0.06783407],\n",
              "       [0.16118252, 0.83881754],\n",
              "       [0.93102384, 0.06897613],\n",
              "       [0.92319053, 0.07680939],\n",
              "       [0.94528204, 0.05471797],\n",
              "       [0.8728254 , 0.12717462],\n",
              "       [0.08428376, 0.91571623],\n",
              "       [0.93045175, 0.06954822],\n",
              "       [0.932741  , 0.06725905],\n",
              "       [0.9068043 , 0.09319576],\n",
              "       [0.8482916 , 0.15170836],\n",
              "       [0.89446306, 0.10553695],\n",
              "       [0.26013735, 0.73986256],\n",
              "       [0.8834985 , 0.11650154],\n",
              "       [0.06573547, 0.93426454],\n",
              "       [0.12888841, 0.8711117 ],\n",
              "       [0.74659455, 0.25340548],\n",
              "       [0.89591116, 0.10408888],\n",
              "       [0.7071976 , 0.29280242],\n",
              "       [0.9284538 , 0.07154623],\n",
              "       [0.80875045, 0.19124953],\n",
              "       [0.9416987 , 0.05830131],\n",
              "       [0.87001765, 0.12998235],\n",
              "       [0.9192758 , 0.08072414],\n",
              "       [0.04105929, 0.9589407 ],\n",
              "       [0.9513914 , 0.0486086 ],\n",
              "       [0.9006201 , 0.0993799 ],\n",
              "       [0.04504252, 0.9549575 ],\n",
              "       [0.5790178 , 0.42098212],\n",
              "       [0.96966326, 0.03033677],\n",
              "       [0.04089729, 0.95910275],\n",
              "       [0.34641197, 0.65358806],\n",
              "       [0.8979681 , 0.10203191],\n",
              "       [0.36760348, 0.63239664],\n",
              "       [0.0762426 , 0.92375743],\n",
              "       [0.22948244, 0.7705176 ],\n",
              "       [0.06249394, 0.9375061 ],\n",
              "       [0.91772914, 0.08227087],\n",
              "       [0.9010789 , 0.09892114],\n",
              "       [0.06493533, 0.9350646 ],\n",
              "       [0.81656027, 0.1834397 ],\n",
              "       [0.97549456, 0.02450548],\n",
              "       [0.03569718, 0.96430284],\n",
              "       [0.09005105, 0.90994895],\n",
              "       [0.93995285, 0.06004716],\n",
              "       [0.91049767, 0.08950239],\n",
              "       [0.91956633, 0.08043375],\n",
              "       [0.89699745, 0.10300257],\n",
              "       [0.8937236 , 0.10627633],\n",
              "       [0.07805727, 0.92194265],\n",
              "       [0.13793182, 0.8620682 ],\n",
              "       [0.76071817, 0.2392818 ],\n",
              "       [0.04107223, 0.95892775],\n",
              "       [0.03133921, 0.9686607 ],\n",
              "       [0.88649434, 0.11350562],\n",
              "       [0.5183581 , 0.48164195],\n",
              "       [0.11914033, 0.8808597 ],\n",
              "       [0.8799948 , 0.12000521],\n",
              "       [0.03589323, 0.96410674],\n",
              "       [0.9008135 , 0.09918647],\n",
              "       [0.44461936, 0.55538064],\n",
              "       [0.9695925 , 0.03040745],\n",
              "       [0.20373668, 0.79626334],\n",
              "       [0.8971134 , 0.10288652],\n",
              "       [0.8944256 , 0.10557448],\n",
              "       [0.91936755, 0.08063238],\n",
              "       [0.6181162 , 0.38188374],\n",
              "       [0.91973406, 0.08026595],\n",
              "       [0.43733355, 0.5626665 ],\n",
              "       [0.9299148 , 0.0700852 ],\n",
              "       [0.22841105, 0.7715889 ],\n",
              "       [0.10827965, 0.89172035],\n",
              "       [0.9896265 , 0.01037346],\n",
              "       [0.06962916, 0.9303708 ],\n",
              "       [0.08569562, 0.9143043 ],\n",
              "       [0.5427476 , 0.45725238],\n",
              "       [0.49268073, 0.5073193 ],\n",
              "       [0.05538013, 0.94461983],\n",
              "       [0.9861578 , 0.01384223],\n",
              "       [0.97948927, 0.02051074],\n",
              "       [0.05755457, 0.9424455 ],\n",
              "       [0.90781724, 0.09218281],\n",
              "       [0.03881862, 0.96118146],\n",
              "       [0.9579828 , 0.04201717],\n",
              "       [0.87506497, 0.12493496],\n",
              "       [0.9578155 , 0.0421844 ],\n",
              "       [0.9638453 , 0.03615466],\n",
              "       [0.05907083, 0.9409291 ],\n",
              "       [0.22261581, 0.77738416],\n",
              "       [0.89368963, 0.10631031],\n",
              "       [0.06165916, 0.9383409 ],\n",
              "       [0.92649627, 0.07350376],\n",
              "       [0.05527294, 0.9447271 ],\n",
              "       [0.93570846, 0.06429153],\n",
              "       [0.10959723, 0.8904028 ],\n",
              "       [0.9186907 , 0.08130925],\n",
              "       [0.44879818, 0.5512018 ],\n",
              "       [0.92407036, 0.07592962],\n",
              "       [0.03650609, 0.96349394],\n",
              "       [0.02825646, 0.9717435 ],\n",
              "       [0.93815243, 0.06184758],\n",
              "       [0.04382276, 0.95617723],\n",
              "       [0.978599  , 0.02140098],\n",
              "       [0.9374421 , 0.06255784],\n",
              "       [0.9055112 , 0.09448879],\n",
              "       [0.12609413, 0.87390584],\n",
              "       [0.91014296, 0.08985706],\n",
              "       [0.9163318 , 0.0836682 ],\n",
              "       [0.7761274 , 0.22387263],\n",
              "       [0.898971  , 0.10102893],\n",
              "       [0.9057274 , 0.09427264],\n",
              "       [0.88900304, 0.11099693],\n",
              "       [0.06963681, 0.9303632 ],\n",
              "       [0.04277571, 0.95722425],\n",
              "       [0.06520567, 0.93479437],\n",
              "       [0.12816134, 0.87183857],\n",
              "       [0.7432248 , 0.2567752 ],\n",
              "       [0.8754553 , 0.12454475],\n",
              "       [0.72215754, 0.27784252],\n",
              "       [0.79014844, 0.20985153],\n",
              "       [0.23708406, 0.7629159 ],\n",
              "       [0.9006622 , 0.09933785],\n",
              "       [0.0782834 , 0.92171663],\n",
              "       [0.04745422, 0.9525458 ],\n",
              "       [0.19801684, 0.8019832 ],\n",
              "       [0.9432038 , 0.05679611],\n",
              "       [0.78843117, 0.21156871],\n",
              "       [0.84487957, 0.15512045],\n",
              "       [0.9006785 , 0.09932154],\n",
              "       [0.9910019 , 0.00899807],\n",
              "       [0.9704897 , 0.02951029],\n",
              "       [0.9051654 , 0.09483468],\n",
              "       [0.07974409, 0.920256  ],\n",
              "       [0.9373381 , 0.06266193],\n",
              "       [0.9207749 , 0.07922514],\n",
              "       [0.93185633, 0.06814362],\n",
              "       [0.13277599, 0.867224  ],\n",
              "       [0.04504252, 0.9549575 ],\n",
              "       [0.9880241 , 0.01197595],\n",
              "       [0.950114  , 0.04988594],\n",
              "       [0.89085037, 0.10914957],\n",
              "       [0.9870089 , 0.012991  ],\n",
              "       [0.35763088, 0.64236915],\n",
              "       [0.9370184 , 0.06298155],\n",
              "       [0.9051146 , 0.09488548],\n",
              "       [0.9196807 , 0.08031931],\n",
              "       [0.06918129, 0.9308187 ],\n",
              "       [0.0587726 , 0.9412274 ],\n",
              "       [0.8518617 , 0.14813828],\n",
              "       [0.06800856, 0.93199146],\n",
              "       [0.66916054, 0.33083948],\n",
              "       [0.8483163 , 0.1516837 ],\n",
              "       [0.87203485, 0.12796517],\n",
              "       [0.97336054, 0.02663946],\n",
              "       [0.09071216, 0.90928787],\n",
              "       [0.6666667 , 0.33333334],\n",
              "       [0.09723475, 0.9027652 ],\n",
              "       [0.05303374, 0.94696623],\n",
              "       [0.0490999 , 0.95090014],\n",
              "       [0.9767843 , 0.02321565],\n",
              "       [0.9589568 , 0.04104318],\n",
              "       [0.9069453 , 0.09305468],\n",
              "       [0.91861296, 0.0813871 ],\n",
              "       [0.9562723 , 0.04372771],\n",
              "       [0.7831952 , 0.21680482],\n",
              "       [0.12614223, 0.8738578 ],\n",
              "       [0.8943135 , 0.10568646],\n",
              "       [0.878559  , 0.12144094],\n",
              "       [0.92327905, 0.07672089],\n",
              "       [0.9361393 , 0.06386074],\n",
              "       [0.09280939, 0.90719056],\n",
              "       [0.8722662 , 0.12773384],\n",
              "       [0.87264663, 0.12735334],\n",
              "       [0.9164049 , 0.08359516],\n",
              "       [0.91604084, 0.08395918],\n",
              "       [0.9187842 , 0.08121586],\n",
              "       [0.94542694, 0.05457302],\n",
              "       [0.9365991 , 0.06340086],\n",
              "       [0.13536245, 0.86463755],\n",
              "       [0.0534149 , 0.9465851 ],\n",
              "       [0.8546874 , 0.14531256],\n",
              "       [0.59399337, 0.40600666],\n",
              "       [0.8289048 , 0.17109513],\n",
              "       [0.0809072 , 0.91909283],\n",
              "       [0.9330803 , 0.06691968],\n",
              "       [0.8590436 , 0.14095643],\n",
              "       [0.7259696 , 0.27403036],\n",
              "       [0.1384608 , 0.8615392 ],\n",
              "       [0.03518141, 0.96481866],\n",
              "       [0.34785718, 0.6521429 ],\n",
              "       [0.72248715, 0.2775128 ],\n",
              "       [0.9279697 , 0.07203042],\n",
              "       [0.88216925, 0.11783069],\n",
              "       [0.9863839 , 0.0136161 ],\n",
              "       [0.89583147, 0.10416853],\n",
              "       [0.8770502 , 0.12294983],\n",
              "       [0.883762  , 0.11623797],\n",
              "       [0.8773117 , 0.12268833],\n",
              "       [0.04969255, 0.9503074 ],\n",
              "       [0.9588699 , 0.04113015],\n",
              "       [0.04107223, 0.95892775],\n",
              "       [0.780231  , 0.219769  ],\n",
              "       [0.5301904 , 0.4698096 ],\n",
              "       [0.89641744, 0.10358257],\n",
              "       [0.13793182, 0.8620682 ],\n",
              "       [0.805353  , 0.194647  ],\n",
              "       [0.87841374, 0.12158625],\n",
              "       [0.0398986 , 0.9601013 ],\n",
              "       [0.935666  , 0.06433403],\n",
              "       [0.9030436 , 0.09695634],\n",
              "       [0.9270599 , 0.07294007],\n",
              "       [0.9135033 , 0.08649668],\n",
              "       [0.9863839 , 0.0136161 ],\n",
              "       [0.4271755 , 0.5728245 ],\n",
              "       [0.9887    , 0.01130001],\n",
              "       [0.9236115 , 0.07638855],\n",
              "       [0.96654093, 0.03345902],\n",
              "       [0.02666114, 0.97333884],\n",
              "       [0.6181162 , 0.38188374],\n",
              "       [0.13529453, 0.86470544],\n",
              "       [0.8980039 , 0.10199617],\n",
              "       [0.05682018, 0.94317985],\n",
              "       [0.90387344, 0.09612657],\n",
              "       [0.25777707, 0.7422229 ],\n",
              "       [0.02507673, 0.9749232 ],\n",
              "       [0.98560077, 0.01439917],\n",
              "       [0.9638453 , 0.03615466],\n",
              "       [0.9055112 , 0.09448879],\n",
              "       [0.19559804, 0.804402  ],\n",
              "       [0.8806928 , 0.11930717],\n",
              "       [0.0388636 , 0.9611364 ],\n",
              "       [0.82159805, 0.17840193],\n",
              "       [0.9144167 , 0.08558332],\n",
              "       [0.15915391, 0.8408461 ],\n",
              "       [0.5327907 , 0.4672093 ],\n",
              "       [0.15856916, 0.84143084],\n",
              "       [0.09644876, 0.9035513 ],\n",
              "       [0.9286806 , 0.07131933],\n",
              "       [0.05348942, 0.94651055],\n",
              "       [0.20373668, 0.79626334],\n",
              "       [0.87479645, 0.12520348],\n",
              "       [0.10690712, 0.8930928 ],\n",
              "       [0.07573125, 0.9242688 ],\n",
              "       [0.8748107 , 0.12518926],\n",
              "       [0.898665  , 0.10133507],\n",
              "       [0.02526025, 0.97473973],\n",
              "       [0.8895406 , 0.1104594 ],\n",
              "       [0.89388293, 0.10611701],\n",
              "       [0.04569246, 0.95430756],\n",
              "       [0.03847211, 0.9615279 ],\n",
              "       [0.06141084, 0.93858916],\n",
              "       [0.87218565, 0.12781444],\n",
              "       [0.79792047, 0.20207949],\n",
              "       [0.5790178 , 0.42098212],\n",
              "       [0.93258876, 0.06741123],\n",
              "       [0.90874285, 0.09125719],\n",
              "       [0.17722934, 0.8227707 ],\n",
              "       [0.11449845, 0.88550156],\n",
              "       [0.85947776, 0.1405222 ],\n",
              "       [0.06017482, 0.9398252 ],\n",
              "       [0.93968403, 0.06031599],\n",
              "       [0.90117604, 0.09882393],\n",
              "       [0.9100376 , 0.08996248],\n",
              "       [0.6212404 , 0.37875956],\n",
              "       [0.8289048 , 0.17109513],\n",
              "       [0.03337052, 0.9666295 ],\n",
              "       [0.48531163, 0.51468843],\n",
              "       [0.98225224, 0.01774779],\n",
              "       [0.92382   , 0.07618   ],\n",
              "       [0.05388832, 0.9461116 ],\n",
              "       [0.93158245, 0.06841752],\n",
              "       [0.06628307, 0.93371695],\n",
              "       [0.89997756, 0.10002248],\n",
              "       [0.8828533 , 0.11714669],\n",
              "       [0.07375119, 0.92624885],\n",
              "       [0.812871  , 0.18712907],\n",
              "       [0.07573125, 0.9242688 ],\n",
              "       [0.8661996 , 0.13380042],\n",
              "       [0.70956284, 0.29043716],\n",
              "       [0.871506  , 0.12849411],\n",
              "       [0.82830906, 0.17169093],\n",
              "       [0.90804136, 0.09195859],\n",
              "       [0.08639428, 0.91360575],\n",
              "       [0.36760348, 0.63239664],\n",
              "       [0.09077356, 0.9092265 ],\n",
              "       [0.04643793, 0.9535621 ],\n",
              "       [0.12762754, 0.8723725 ],\n",
              "       [0.99007094, 0.00992902],\n",
              "       [0.03322615, 0.9667739 ],\n",
              "       [0.9502623 , 0.04973768],\n",
              "       [0.93804973, 0.06195027],\n",
              "       [0.19854708, 0.80145293]], dtype=float32)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
