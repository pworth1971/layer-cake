{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCFWazYMSLJz",
        "outputId": "a4ab08eb-1487-475e-c7a8-3e5b1dd6f7ca"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "#!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "09TscoDsSPpL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import math\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import tensorflow as tf\n",
        "#import tensorflow.keras.backend as K\n",
        "import tokenizers\n",
        "#from transformers import BertTokenizer, TFBertModel, BertForSequenceClassification, BertConfig\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7mM99kOWSZI2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on CPU\n",
            "Number of GPUs: 0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f'Running on GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Running on CPU')\n",
        "\n",
        "# Number of GPUs available\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print('Number of GPUs:', num_gpus)\n",
        "\n",
        "# If using multiple GPUs, use DataParallel or DistributedDataParallel\n",
        "if num_gpus > 1:\n",
        "    model = torch.nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4fGeJVk6SdKT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "Model: bert-base-uncased, Max Length: 256, Batch Size: 8, Epochs: 3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "ARTIFACTS_PATH = '../artifacts/'\n",
        "\n",
        "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "EPOCHS = 3\n",
        "\n",
        "if not os.path.exists(ARTIFACTS_PATH):\n",
        "    os.makedirs(ARTIFACTS_PATH)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "ARTIFACTS_PATH = '../../artifacts/'\n",
        "\n",
        "# Determine the number of devices (GPUs/CPUs)\n",
        "if torch.cuda.is_available():\n",
        "    num_replicas = torch.cuda.device_count()\n",
        "    print(f'Using {num_replicas} GPU(s)')\n",
        "else:\n",
        "    num_replicas = 1  # Use CPU or single GPU\n",
        "    print('Using CPU')\n",
        "\n",
        "# Set the batch size and number of epochs\n",
        "BATCH_SIZE = 8 * num_replicas\n",
        "EPOCHS = 3\n",
        "\n",
        "# Create the artifacts directory if it doesn't exist\n",
        "if not os.path.exists(ARTIFACTS_PATH):\n",
        "    os.makedirs(ARTIFACTS_PATH)\n",
        "\n",
        "print(f'Model: {MODEL_NAME}, Max Length: {MAX_LEN}, Batch Size: {BATCH_SIZE}, Epochs: {EPOCHS}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import model_selection, svm, metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, hamming_loss, jaccard_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from pprint import pprint\n",
        "import argparse\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import cufflinks as cf\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#\n",
        "# we assume everything runs from bin directory\n",
        "#\n",
        "PICKLE_DIR = '../../pickles/'\n",
        "OUT_DIR = '../../out/'\n",
        "DATASET_DIR = '../../datasets/'\n",
        "\n",
        "\n",
        "#dataset_available = {'reuters21578', '20newsgroups', 'ohsumed', 'rcv1'}\n",
        "dataset_available = {'20newsgroups', 'bbc-news'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#\n",
        "# preprocessing utility methods\n",
        "#\n",
        "\n",
        "def create_confusion_matrix(y_test, y_pred, title, file_name=OUT_DIR+'svm_20newsgroups_confusion_matrix_best_model_table.png', debug=False):\n",
        "\n",
        "    print(\"Creating confusion matrix...\")\n",
        "\n",
        "    # Assuming y_test and y_pred_best are already defined\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Plotting the confusion matrix as a table with numbers\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))  # Increase the width and height of the figure\n",
        "\n",
        "    # Hide axes\n",
        "    ax.xaxis.set_visible(False) \n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.set_frame_on(False)\n",
        "\n",
        "    # Create the table with smaller font sizes and adjusted scale\n",
        "    table = ax.table(\n",
        "        cellText=conf_matrix,\n",
        "        rowLabels=[f'Actual {i}' for i in range(conf_matrix.shape[0])],\n",
        "        colLabels=[f'Predicted {i}' for i in range(conf_matrix.shape[1])],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "\n",
        "    # Adjust the font size and layout\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)  # Reduced font size for better fitting\n",
        "    table.scale(1.2, 1.2)\n",
        "\n",
        "    # Add a title with centered text\n",
        "    plt.title(title, fontsize=16, pad=20)\n",
        "\n",
        "    # Adjust layout to add more padding around the plot\n",
        "    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)  # Increase padding on the left\n",
        "\n",
        "    # Save the plot to a file\n",
        "    confusion_matrix_filename = file_name\n",
        "    plt.savefig(confusion_matrix_filename, bbox_inches='tight')  # Ensure everything is saved in the output file\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Confusion matrix saved as {confusion_matrix_filename}\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Plain text explanation of the confusion matrix\n",
        "    if debug:\n",
        "        print(\"\\nHow to read this confusion matrix:\")\n",
        "        print(\"------------------------------------------------------\")\n",
        "        print(\"The confusion matrix shows the performance of the classification model.\")\n",
        "        print(\"Each row of the matrix represents the actual classes, while each column represents the predicted classes.\")\n",
        "        print(\"Values on the diagonal (from top-left to bottom-right) represent correct predictions (true positives and true negatives).\")\n",
        "        print(\"Values outside the diagonal represent incorrect predictions (false positives and false negatives).\")\n",
        "        print(\"\\nAccuracy Score: {:.2f}%\".format(accuracy * 100))\n",
        "        \n",
        "        print(\"\\nConfusion Matrix Values:\")\n",
        "        for i in range(len(conf_matrix)):\n",
        "            print(f\"Actual class {i}:\")\n",
        "            for j in range(len(conf_matrix[i])):\n",
        "                print(f\"  Predicted as class {j}: {conf_matrix[i][j]}\")\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "#\n",
        "# Utility functions for preprocessing data\n",
        "#\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "def missing_values(df):\n",
        "    \"\"\"\n",
        "    Calculate the percentage of missing values for each column in a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "    df (pd.DataFrame): The input DataFrame to analyze.\n",
        "    \n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing the total count and percentage of missing values for each column.\n",
        "    \"\"\"\n",
        "    # Calculate total missing values and their percentage\n",
        "    total = df.isnull().sum()\n",
        "    percent = (total / len(df) * 100)\n",
        "    \n",
        "    # Create a DataFrame with the results\n",
        "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    \n",
        "    # Sort the DataFrame by percentage of missing values (descending)\n",
        "    missing_data = missing_data.sort_values('Percent', ascending=False)\n",
        "    \n",
        "    # Filter out columns with no missing values\n",
        "    missing_data = missing_data[missing_data['Total'] > 0]\n",
        "    \n",
        "    print(\"Columns with missing values:\")\n",
        "    print(missing_data)\n",
        "    \n",
        "    return missing_data\n",
        "\n",
        "\n",
        "def remove_punctuation(x):\n",
        "    punctuationfree=\"\".join([i for i in x if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "\n",
        "# Function to lemmatize text with memory optimization\n",
        "def lemmatization(texts, chunk_size=1000):\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    \n",
        "    num_chunks = len(texts) // chunk_size + 1\n",
        "    #print(f\"Number of chunks: {num_chunks}\")\n",
        "    for i in range(num_chunks):\n",
        "        chunk = texts[i*chunk_size:(i+1)*chunk_size]\n",
        "        texts[i*chunk_size:(i+1)*chunk_size] = [' '.join([lmtzr.lemmatize(word) for word in text.split()]) for text in chunk]\n",
        "    \n",
        "    return texts\n",
        "\n",
        "\n",
        "def preprocessDataset(train_text):\n",
        "    # Ensure input is string\n",
        "    train_text = str(train_text)\n",
        "    \n",
        "    # Word tokenization using NLTK's word_tokenize\n",
        "    tokenized_train_set = word_tokenize(train_text.lower())\n",
        "    \n",
        "    # Stop word removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stopwordremove = [i for i in tokenized_train_set if i not in stop_words]\n",
        "    \n",
        "    # Join words into sentence\n",
        "    stopwordremove_text = ' '.join(stopwordremove)\n",
        "    \n",
        "    # Remove numbers\n",
        "    numberremove_text = ''.join(c for c in stopwordremove_text if not c.isdigit())\n",
        "    \n",
        "    # Stemming using NLTK's PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stem_input = word_tokenize(numberremove_text)\n",
        "    stem_text = ' '.join([stemmer.stem(word) for word in stem_input])\n",
        "    \n",
        "    # Lemmatization using NLTK's WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    def get_wordnet_pos(word):\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "    \n",
        "    lem_input = word_tokenize(stem_text)\n",
        "    lem_text = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in lem_input])\n",
        "    \n",
        "    return lem_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#\n",
        "# load_data\n",
        "#\n",
        "def load_data(dataset='20newsgroups'):\n",
        "\n",
        "    print(f\"Loading data set {dataset}...\")\n",
        "\n",
        "    if (dataset == '20newsgroups'):\n",
        "        \n",
        "        print(\"Loading 20 newsgroups dataset...\")\n",
        "\n",
        "        # Fetch the 20 newsgroups dataset\n",
        "        newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "        # Create a DataFrame from the Bunch object\n",
        "        df = pd.DataFrame({\n",
        "            'text': newsgroups.data,\n",
        "            'category': newsgroups.target\n",
        "        })\n",
        "\n",
        "        # Add category names\n",
        "        df['category_name'] = [newsgroups.target_names[i] for i in df['category']]\n",
        "\n",
        "        print(f\"Number of documents: {len(df)}\")\n",
        "        print(f\"Number of categories: {len(df['category'].unique())}\")\n",
        "        print(f\"Number of category names: {len(df['category_name'].unique())}\")\n",
        "        #pprint(list(df.target_names))\n",
        "        #pprint(list(df.category_name))\n",
        "\n",
        "        #df = df['category'].unique()    \n",
        "\n",
        "        missing_values_df = missing_values(df)\n",
        "        print(f\"missing values:\", missing_values_df)\n",
        "\n",
        "        ### Start of Text Pre-processing\n",
        "        print(\"preproccessing...\")\n",
        "\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('averaged_perceptron_tagger')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('omw-1.4')\n",
        "\n",
        "        string.punctuation\n",
        "\n",
        "        ### 2. To LowerCase\n",
        "\n",
        "        df['CleanedText'] = (df.text.apply(lambda x: x.lower()))\n",
        "\n",
        "        ### 3. Removing Numbers and Special Characters including XXXXXX\n",
        "\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('\\W+', ' ', x)))\n",
        "        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
        "\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub(regex, '', x)))\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('xxxx', '', x)))\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('xx', '', x)))\n",
        "\n",
        "        print(\"removing punctuation...\")\n",
        "\n",
        "        df['CleanedText'] =  (df.CleanedText.apply(lambda x: remove_punctuation(x)))\n",
        "\n",
        "        ### 5. Tokenization\n",
        "        #data['TokenizedText'] =  (data.CleanedText.apply(lambda x: re.split('W+',x)))\n",
        "\n",
        "        print(\"removing stopwords...\")\n",
        "        from nltk.corpus import stopwords\n",
        "        nltk.download('stopwords')\n",
        "        stopwords = set(stopwords.words(\"english\"))\n",
        "        df['CleanedText'] = df.CleanedText.apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n",
        "        print(\"Stopwords removed\")\n",
        "        #print(df['CleanedText'][0])\n",
        "\n",
        "        ## TFIDF already tokenizes the text so no need to tokenize it here\n",
        "        # from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "        # data2['TokenizedText'] = data2.CleanedText.apply(word_tokenize)\n",
        "\n",
        "\n",
        "        ### 7. Text Normalization  [Lemmatization] -->better than Stemming since it returns actual words\n",
        "        ## lemmatization is an intelligent operation that uses dictionaries\n",
        "\n",
        "        print(\"Lemmatizing...\")\n",
        "\n",
        "        df['LemmatizedText'] = lemmatization(df['CleanedText'])\n",
        "\n",
        "        print(\"Lemmatized\")\n",
        "        #print(df['CleanedText'][0])\n",
        "        #print(df['LemmatizedText'][0])\n",
        "\n",
        "        print(\"Tokenizing...\")\n",
        "        \n",
        "        # Tokenize the text data\n",
        "        df['tokenized'] = df['text'].str.lower().apply(nltk.word_tokenize)\n",
        "\n",
        "        #return df, df['category'].unique()\n",
        "        return df\n",
        "    \n",
        "    elif (dataset == 'bbc-news'):\n",
        "\n",
        "        print(\"Loading BBC News dataset...\")\n",
        "\n",
        "        for dirname, _, filenames in os.walk(DATASET_DIR+'bbc-news'):\n",
        "            for filename in filenames:\n",
        "                print(os.path.join(dirname, filename))\n",
        "\n",
        "        train_set = pd.read_csv(DATASET_DIR+'bbc-news/BBC News Train.csv')\n",
        "        test_set = pd.read_csv(DATASET_DIR+'bbc-news/BBC News Test.csv')\n",
        "\n",
        "        print(\"train_set:\", train_set.shape)\n",
        "        print(train_set.head())\n",
        "        print(\"test_set:\", test_set.shape)\n",
        "        print(test_set.head())\n",
        "\n",
        "        target_category = train_set['Category'].unique()\n",
        "        print(\"target categories:\", target_category)\n",
        "\n",
        "        train_set['categoryId'] = train_set['Category'].factorize()[0]\n",
        "        \n",
        "        category = train_set[[\"Category\",\"categoryId\"]].drop_duplicates().sort_values('categoryId')\n",
        "        print(\"after de-duping:\", category)\n",
        "\n",
        "        print(train_set.groupby('Category').categoryId.count())\n",
        "\n",
        "        text = train_set[\"Text\"] \n",
        "        print(\"text:\\n\", text.head())\n",
        "\n",
        "        category = train_set[\"Category\"]\n",
        "        print(\"categories:\\n\", category.head())\n",
        "\n",
        "        print(\"preprocessing...\")\n",
        "        train_set['Text'] = train_set['Text'].apply(preprocessDataset)\n",
        "        text = train_set['Text']\n",
        "        category = train_set['Category']\n",
        "        print(\"text:\\n\", type(text), text.shape)\n",
        "        print(text.head())\n",
        "\n",
        "        #return text, category\n",
        "        return train_set\n",
        "    else:\n",
        "        print(f\"Dataset '{dataset}' not available.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_svm_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    print(\"Training default Support Vector Machine model...\")\n",
        "    \n",
        "    default_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('lr', LinearSVC(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    default_pipeline.fit(X_train, y_train)\n",
        "    y_pred_default = default_pipeline.predict(X_test)\n",
        "\n",
        "    print(\"\\nDefault Support Vector Mechine Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_default):.4f}\")\n",
        "    print(classification_report(y_true=y_test, y_pred=y_pred_default, digits=4))\n",
        "\n",
        "    if (args.optimc):\n",
        "\n",
        "        # Optimize Support Vector Machine with GridSearchCV\n",
        "        print(\"Optimizing Support Vector Machine model with GridSearchCV...\")\n",
        "\n",
        "        # Define the pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('svm', LinearSVC(max_iter=1000))\n",
        "        ])\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],     # Unigrams, bigrams, or trigrams\n",
        "            'tfidf__use_idf': [True, False],                    # Whether to use IDF\n",
        "            'tfidf__sublinear_tf': [True, False],               # Sublinear term frequency\n",
        "            'svm__penalty': ['l1', 'l2'],                       # Regularization method\n",
        "            'svm__loss': ['hinge', 'squared_hinge'],            # Loss function\n",
        "            'svm__multi_class': ['ovr', 'crammer_singer'],      # Multi-class strategy\n",
        "            'svm__class_weight': [None, 'balanced'],            # Class weights\n",
        "            'svm__C': np.logspace(-3, 3, 7)                     # Regularization parameter   \n",
        "        }\n",
        "\n",
        "        print(\"param_grid:\", param_grid)\n",
        "\n",
        "        cross_validation = StratifiedKFold()\n",
        "\n",
        "        scorers = {\n",
        "            'accuracy_score': make_scorer(accuracy_score),\n",
        "            'f1_score': make_scorer(f1_score, average='micro'),\n",
        "            'recall_score': make_scorer(recall_score, average='micro'),\n",
        "            'precision_score': make_scorer(precision_score, average='micro'),\n",
        "            'hamming_loss': make_scorer(hamming_loss),\n",
        "            'jaccard_score': make_scorer(jaccard_score, average='micro')\n",
        "            }\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            n_jobs=-1, \n",
        "            estimator=pipeline,\n",
        "            refit='f1_score',\n",
        "            param_grid=param_grid,\n",
        "            cv=cross_validation,\n",
        "            #scoring=scoring\n",
        "            scoring=scorers,\n",
        "            return_train_score=True         # ensure train scores are calculated\n",
        "            )\n",
        "\n",
        "        # Fit the model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "        print(\"best_estimator:\", grid_search.best_estimator_)\n",
        "        print('Best score: {}'.format(grid_search.best_score_))\n",
        "        print(\"cv_results_:\", grid_search.cv_results_)\n",
        "\n",
        "        results = grid_search.cv_results_\n",
        "\n",
        "        if (args.plot):\n",
        "\n",
        "            print(\"Plotting the results...\")\n",
        "\n",
        "            # Define the metrics we want to plot\n",
        "            metrics_to_plot = ['accuracy_score', 'f1_score', 'recall_score', 'precision_score', 'hamming_loss']\n",
        "\n",
        "            # Iterate over each metric to create a separate plot\n",
        "            for metric in metrics_to_plot:\n",
        "                traces = []\n",
        "\n",
        "                print(f\"Plotting {metric}...\")\n",
        "\n",
        "                for sample in [\"train\", \"test\"]:\n",
        "\n",
        "                    key_mean = f\"mean_{sample}_{metric}\"\n",
        "                    key_std = f\"std_{sample}_{metric}\"\n",
        "\n",
        "                    print(f\"Plotting {key_mean}...\")\n",
        "                    print(f\"Plotting {key_std}...\")\n",
        "\n",
        "                    # Directly use the keys without conditional check\n",
        "                    sample_score_mean = np.nan_to_num(np.array(results[key_mean]) * 100)  # Convert to percentage and handle NaN\n",
        "                    sample_score_std = np.nan_to_num(np.array(results[key_std]) * 100)  # Convert to percentage and handle NaN\n",
        "\n",
        "                    x_axis = np.linspace(0, 100, len(sample_score_mean))\n",
        "\n",
        "                    # Create the trace for Plotly\n",
        "                    traces.append(\n",
        "                        go.Scatter(\n",
        "                            x=x_axis,\n",
        "                            y=sample_score_mean,\n",
        "                            mode='lines+markers',\n",
        "                            name=f\"{metric} ({sample})\",\n",
        "                            line=dict(dash='dash' if sample == 'train' else 'solid'),\n",
        "                            error_y=dict(\n",
        "                                type='data',\n",
        "                                array=sample_score_std,\n",
        "                                visible=True\n",
        "                            ),\n",
        "                            hoverinfo='x+y+name'\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Define the layout of the plot\n",
        "                layout = go.Layout(\n",
        "                    title={'text': f\"Training and Test Scores for {metric.capitalize()}\",\n",
        "                        'y':0.9,\n",
        "                        'x':0.5,\n",
        "                        'xanchor': 'center',\n",
        "                        'yanchor': 'top'},\n",
        "                    xaxis=dict(title=\"Training Sample Percentage (%)\"),\n",
        "                    yaxis=dict(title=\"Score (%)\", range=[0, 100]),\n",
        "                    hovermode='closest'\n",
        "                )\n",
        "\n",
        "                # Create the figure\n",
        "                fig = go.Figure(data=traces, layout=layout)\n",
        "\n",
        "                # Write the plot to an HTML file\n",
        "                filename = f'{OUT_DIR}training_test_scores_{metric}.html'\n",
        "                pyo.plot(fig, filename=filename)\n",
        "\n",
        "                print(f\"Saved plot for {metric} as {filename}\")\n",
        "\n",
        "        # Extract the best estimator from the GridSearchCV\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # Predict on the test set using the best model\n",
        "        y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "        print(\"Accuracy best score:\", metrics.accuracy_score(y_test, y_pred_best))\n",
        "        print(classification_report(y_true=y_test, y_pred=y_pred_best, digits=4))\n",
        "\n",
        "\n",
        "def run_lr_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    # Default Logistic Regression Model\n",
        "    print(\"Training default Logistic Regression model...\")\n",
        "    default_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('lr', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    default_pipeline.fit(X_train, y_train)\n",
        "    y_pred_default = default_pipeline.predict(X_test)\n",
        "\n",
        "    print(\"\\nDefault Logistic Regression Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_default):.4f}\")\n",
        "    print(classification_report(y_true=y_test, y_pred=y_pred_default, digits=4))\n",
        "\n",
        "    if (args.optimc):\n",
        "        # Optimize Logistic Regression with GridSearchCV\n",
        "        print(\"Optimizing Logistic Regression model with GridSearchCV...\")\n",
        "\n",
        "        # Define the pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('lr', LogisticRegression(max_iter=1000))\n",
        "        ])\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],     # Unigrams, bigrams, or trigrams\n",
        "            'tfidf__use_idf': [True, False],                    # Whether to use IDF\n",
        "            'tfidf__sublinear_tf': [True, False],               # Sublinear term frequency\n",
        "            'lr__C': [0.01, 0.1, 1, 10, 100],                   # Inverse of regularization strength\n",
        "            'lr__penalty': ['l2'],                              # Regularization method (L2 Ridge)\n",
        "            'lr__solver': ['liblinear', 'lbfgs']                # Solver types\n",
        "        }\n",
        "\n",
        "        print(\"param_grid:\", param_grid)\n",
        "\n",
        "        # Define scorers\n",
        "        scorers = {\n",
        "            'accuracy_score': make_scorer(accuracy_score),\n",
        "            'f1_score': make_scorer(f1_score, average='micro'),\n",
        "            'recall_score': make_scorer(recall_score, average='micro'),\n",
        "            'precision_score': make_scorer(precision_score, average='micro')\n",
        "        }\n",
        "\n",
        "        # Initialize GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=scorers,\n",
        "            refit='f1_score',  # Optimize on F1 Score\n",
        "            cv=StratifiedKFold(n_splits=5),\n",
        "            n_jobs=-1,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Display the best parameters\n",
        "        print('Best parameters found by GridSearchCV:')\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        y_pred_optimized = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "        print(\"\\nOptimized Logistic Regression Model Performance:\")\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred_optimized):.4f}\")\n",
        "        print(classification_report(y_true=y_test, y_pred=y_pred_optimized, digits=4))\n",
        "\n",
        "    if (args.cm):\n",
        "        # Optionally, plot confusion matrix for the optimized model\n",
        "        create_confusion_matrix(\n",
        "            y_test, \n",
        "            y_pred_optimized, \n",
        "            title='Confusion Matrix for Optimized Logistic Regression Model',\n",
        "            file_name=OUT_DIR+'bbc_news_logistic_regression_confusion_matrix.png',\n",
        "            debug=False\n",
        "        )\n",
        "\n",
        "\n",
        "def run_nb_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    print(\"Building default Naive Bayes Classifier...\")\n",
        "\n",
        "    nb = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', MultinomialNB())\n",
        "        ])\n",
        "    \n",
        "    nb.fit(X_train,y_train)\n",
        "\n",
        "    test_predict = nb.predict(X_test)\n",
        "\n",
        "    train_accuracy = round(nb.score(X_train,y_train)*100)\n",
        "    test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
        "\n",
        "    print(\"Naive Bayes Train Accuracy Score : {}% \".format(train_accuracy ))\n",
        "    print(\"Naive Bayes Test Accuracy Score  : {}% \".format(test_accuracy ))\n",
        "    print(classification_report(y_true=test_predict, y_pred=y_test, digits=4))\n",
        "\n",
        "    if (args.optimc):\n",
        "\n",
        "        print(\"Optimizing the model using GridSearchCV...\")\n",
        "\n",
        "        # Define a pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('nb', MultinomialNB())\n",
        "        ])\n",
        "\n",
        "        # Define the parameter grid\n",
        "        param_grid = {\n",
        "            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],         # Unigrams, bigrams, or trigrams\n",
        "            'tfidf__use_idf': [True, False],                        # Whether to use IDF\n",
        "            'tfidf__sublinear_tf': [True, False],                   # Sublinear term frequency\n",
        "            'nb__alpha': [0.1, 0.5, 1.0, 1.5, 2.0],                 # Smoothing parameter for Naive Bayes\n",
        "        }\n",
        "\n",
        "        print(\"param_grid:\", param_grid)\n",
        "\n",
        "        # Define scorers\n",
        "        scorers = {\n",
        "            'accuracy_score': make_scorer(accuracy_score),\n",
        "            'f1_score': make_scorer(f1_score, average='micro'),\n",
        "            'recall_score': make_scorer(recall_score, average='micro'),\n",
        "            'precision_score': make_scorer(precision_score, average='micro'),\n",
        "            'hamming_loss': make_scorer(hamming_loss),\n",
        "            'jaccard_score': make_scorer(jaccard_score, average='micro')\n",
        "        }\n",
        "\n",
        "        # Initialize GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=scorers,\n",
        "            refit='f1_score',                           # Optimize on F1 Score\n",
        "            cv=StratifiedKFold(n_splits=5),\n",
        "            n_jobs=-1,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Display the best parameters\n",
        "        print('Best parameters found by GridSearchCV:')\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "        print(\"\\nBest Estimator's Test Set Performance:\")\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(f\"F1 Score: {f1_score(y_test, y_pred, average='micro'):.4f}\")\n",
        "        print(f\"Recall: {recall_score(y_test, y_pred, average='micro'):.4f}\")\n",
        "        print(f\"Precision: {precision_score(y_test, y_pred, average='micro'):.4f}\")\n",
        "        print(classification_report(y_true=y_test, y_pred=y_pred, digits=4))\n",
        "\n",
        "        if (args.cm):\n",
        "            # Optionally, plot confusion matrix\n",
        "            create_confusion_matrix(\n",
        "                y_test, \n",
        "                y_pred, \n",
        "                title='Confusion Matrix for Optimized Naive Bayes Model',\n",
        "                file_name=OUT_DIR+'bbc_news_naive_bayes_confusion_matrix.png',\n",
        "                debug=False\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_model(X_train, X_test, y_train, y_test, args):\n",
        "\n",
        "    print(\"Running model...\")\n",
        "\n",
        "    # Support Vector Machine Classifier\n",
        "    if (args.learner == 'svm'):\n",
        "        run_svm_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "    # Logistic Regression Classifier\n",
        "    elif (args.learner == 'lr'):\n",
        "        run_lr_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "    # Naive Bayes (MultinomialNB) Classifier\n",
        "    elif (args.learner == 'nb'):\n",
        "        run_nb_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "    elif (args.learner == 'dt'):\n",
        "        print(\"Decision Tree Classifier\")\n",
        "        dt = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('dt', DecisionTreeClassifier())\n",
        "            ])\n",
        "\n",
        "        dt.fit(X_train, y_train)\n",
        "\n",
        "        test_predict = dt.predict(X_test)\n",
        "\n",
        "        train_accuracy = round(dt.score(X_train, y_train)*100)\n",
        "        test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
        "\n",
        "        print(\"Decision Tree Train Accuracy Score : {}% \".format(train_accuracy ))\n",
        "        print(\"Decision Tree Test Accuracy Score  : {}% \".format(test_accuracy ))\n",
        "        print(classification_report(y_true=test_predict, y_pred=y_test, digits=4))\n",
        "\n",
        "    elif (args.learner == 'rf'):\n",
        "\n",
        "        print(\"Random Forest Classifier\")\n",
        "        rfc = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer()),\n",
        "            ('rfc', RandomForestClassifier(n_estimators=100))\n",
        "            ])\n",
        "\n",
        "        rfc.fit(X_train, y_train)\n",
        "\n",
        "        test_predict = rfc.predict(X_test)\n",
        "\n",
        "        train_accuracy = round(rfc.score(X_train, y_train)*100)\n",
        "        test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
        "\n",
        "        print(\"K-Nearest Neighbour Train Accuracy Score : {}% \".format(train_accuracy ))\n",
        "        print(\"K-Nearest Neighbour Test Accuracy Score  : {}% \".format(test_accuracy ))\n",
        "        print(classification_report(y_true=test_predict, y_pred=y_test, digits=4))\n",
        "\n",
        "    else:\n",
        "        print(f\"Invalid learner '{args.learner}'\")\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "# Core processing function\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "def classify(dataset='20newsgrouops', args=None):\n",
        "    \n",
        "    if (args is None):\n",
        "        print(\"No arguments passed.\")\n",
        "        return\n",
        "    \n",
        "    if (args.dataset not in dataset_available):\n",
        "        print(f\"Dataset '{args.dataset}' not available.\")\n",
        "        return\n",
        "    \n",
        "    pickle_file_name=f'{dataset}_{args.mode}_tokenized.pickle'\n",
        "\n",
        "    print(f\"Classifying {dataset}...\")\n",
        "\n",
        "    print(f\"Loading data set {dataset}...\")\n",
        "\n",
        "    # Define the path to the pickle file\n",
        "    pickle_file = PICKLE_DIR + pickle_file_name\n",
        "\n",
        "    if os.path.exists(pickle_file):                                         # if the pickle file exists\n",
        "        \n",
        "        print(f\"Loading tokenized data from '{pickle_file}'...\")\n",
        "        \n",
        "        if (dataset == '20newsgroups'):\n",
        "            # Initialize an empty DataFrame with the desired columns\n",
        "            columns = ['tokenized', 'CleanedText', 'LemmatizedText', 'category', 'category_name', 'text']\n",
        "            \n",
        "            df = pd.DataFrame(columns=columns)\n",
        "            \n",
        "            # Load the data from the pickle file into the DataFrame\n",
        "            with open(pickle_file, 'rb') as f:\n",
        "                df = pickle.load(f)\n",
        "\n",
        "            #categories = df['category'].unique()\n",
        "            \n",
        "        elif (dataset == 'bbc-news'):\n",
        "\n",
        "            columns = []\n",
        "\n",
        "            df = pd.DataFrame(columns=columns)\n",
        "            \n",
        "            with open(pickle_file, 'rb') as f:\n",
        "                df = pickle.load(f)\n",
        "    else:\n",
        "        print(f\"'{pickle_file}' not found, retrieving and preprocessing data set {dataset}...\")\n",
        "\n",
        "        #df, categories = load_data(dataset)\n",
        "        df = load_data(dataset)\n",
        "\n",
        "        print(\"df:\", df.shape)\n",
        "        print(df.head())\n",
        "\n",
        "        # Save the tokenized DataFrame to a pickle file\n",
        "        if (dataset == '20newsgroups'):\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(df[['tokenized', 'CleanedText', 'LemmatizedText', 'category', 'category_name', 'text']], f)\n",
        "\n",
        "        elif (dataset == 'bbc-news'):\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(df[['ArticleId', 'Text', 'Category']], f)\n",
        "                #pickle.dump(df, f)\n",
        "\n",
        "    print(\"Tokenized data loaded, df:\", df.shape)\n",
        "    print(df.head())\n",
        "    #print(\"categories:\", categories)\n",
        "\n",
        "    print(\"Processing data...\")\n",
        "\n",
        "    if (dataset == '20newsgroups'):\n",
        "\n",
        "        print(\"classifying 20 newsgroups...\")\n",
        "\n",
        "        \"\"\"\n",
        "        # POS Tagging and Counting\n",
        "        tagged_titles = df['text'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
        "\n",
        "        def count_tags(title_with_tags):\n",
        "            tag_count = {}\n",
        "            for word, tag in title_with_tags:\n",
        "                tag_count[tag] = tag_count.get(tag, 0) + 1\n",
        "            return tag_count\n",
        "\n",
        "        # Create a DataFrame with POS tag counts\n",
        "        tagged_titles_df = pd.DataFrame(tagged_titles.apply(lambda x: count_tags(x)).tolist()).fillna(0)\n",
        "\n",
        "        # Sum the occurrences of each tag across all documents\n",
        "        tagged_titles_sum = tagged_titles_df.sum().sort_values(ascending=False)\n",
        "\n",
        "        # Plot POS Tag Frequency\n",
        "        trace = go.Bar(x=tagged_titles_sum.index, y=tagged_titles_sum.values)\n",
        "        layout = go.Layout(title='Frequency of POS Tags in IT Support Tickets Dataset', xaxis=dict(title='POS'), yaxis=dict(title='Count'))\n",
        "        fig = go.Figure(data=[trace], layout=layout)\n",
        "\n",
        "        # This will open the plot in the default web browser\n",
        "        pyo.plot(fig, filename='../../out/pos_tag_frequency.html')\n",
        "        \"\"\"\n",
        "\n",
        "        # Feature Extraction and Model Training\n",
        "        print(\"Splitting the dataset...\")\n",
        "\n",
        "        X = df['CleanedText']\n",
        "        y = df['category']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44)\n",
        "\n",
        "        print(\"X_train:\", type(X_train), X_train.shape)\n",
        "        print(\"X_test:\", type(X_test), X_test.shape)\n",
        "        \n",
        "        print(\"Y_train:\", type(y_train), y_train.shape)\n",
        "        print(\"Y_test:\", type(y_test), y_test.shape)\n",
        "\n",
        "        print(\"Running model...\")\n",
        "\n",
        "        run_model(X_train, X_test, y_train, y_test, args)\n",
        "    \n",
        "    elif (dataset == 'bbc-news'):\n",
        "\n",
        "        print(f'classifying bbc-news...')\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['Text'],\n",
        "            df['Category'], \n",
        "            test_size = 0.3, \n",
        "            random_state = 60,\n",
        "            shuffle=True, \n",
        "            stratify=df['Category']\n",
        "            )\n",
        "\n",
        "        print(\"X_train:\", type(X_train), X_train.shape)\n",
        "        print(\"X_test:\", type(X_test), X_test.shape)\n",
        "        \n",
        "        print(\"Y_train:\", type(y_train), y_train.shape)\n",
        "        print(\"Y_test:\", type(y_test), y_test.shape)\n",
        "\n",
        "        print(\"Running model...\")\n",
        "\n",
        "        run_model(X_train, X_test, y_train, y_test, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data set bbc-news...\n",
            "Loading BBC News dataset...\n",
            "../../datasets/bbc-news/BBC News Train.csv\n",
            "../../datasets/bbc-news/.DS_Store\n",
            "../../datasets/bbc-news/BBC News Test.csv\n",
            "../../datasets/bbc-news/BBC News Sample Solution.csv\n",
            "train_set: (1490, 3)\n",
            "   ArticleId                                               Text  Category\n",
            "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
            "1        154  german business confidence slides german busin...  business\n",
            "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
            "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
            "4        917  enron bosses in $168m payout eighteen former e...  business\n",
            "test_set: (735, 2)\n",
            "   ArticleId                                               Text\n",
            "0       1018  qpr keeper day heads for preston queens park r...\n",
            "1       1319  software watching while you work software that...\n",
            "2       1138  d arcy injury adds to ireland woe gordon d arc...\n",
            "3        459  india s reliance family feud heats up the ongo...\n",
            "4       1020  boro suffer morrison injury blow middlesbrough...\n",
            "target categories: ['business' 'tech' 'politics' 'sport' 'entertainment']\n",
            "after de-duping:         Category  categoryId\n",
            "0       business           0\n",
            "3           tech           1\n",
            "5       politics           2\n",
            "6          sport           3\n",
            "7  entertainment           4\n",
            "Category\n",
            "business         336\n",
            "entertainment    273\n",
            "politics         274\n",
            "sport            346\n",
            "tech             261\n",
            "Name: categoryId, dtype: int64\n",
            "text:\n",
            " 0    worldcom ex-boss launches defence lawyers defe...\n",
            "1    german business confidence slides german busin...\n",
            "2    bbc poll indicates economic gloom citizens in ...\n",
            "3    lifestyle  governs mobile choice  faster  bett...\n",
            "4    enron bosses in $168m payout eighteen former e...\n",
            "Name: Text, dtype: object\n",
            "categories:\n",
            " 0    business\n",
            "1    business\n",
            "2    business\n",
            "3        tech\n",
            "4    business\n",
            "Name: Category, dtype: object\n",
            "preprocessing...\n",
            "text:\n",
            " <class 'pandas.core.series.Series'> (1490,)\n",
            "0    worldcom ex-boss launch defenc lawyer defend f...\n",
            "1    german busi confid slide german busi confid fe...\n",
            "2    bbc poll indic econom gloom citizen major nati...\n",
            "3    lifestyl govern mobil choic faster well funkie...\n",
            "4    enron bos $ m payout eighteen former enron dir...\n",
            "Name: Text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "data = load_data('bbc-news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "QswhfKYMT5V2",
        "outputId": "ebb07af4-aeb9-4e0e-a200-8dacc5b52c54"
      },
      "outputs": [],
      "source": [
        "##  Creating a new Dataframe with the cols we're interested In\n",
        "\"\"\"\n",
        "cols = ['FinalText', 'DV_CATEGORY']\n",
        "#cols = ['TokenizedText', 'Product']\n",
        "data = data[cols]\n",
        "\n",
        "\"\"\"\n",
        "data = data.rename(columns = {'Text': 'FinalText', 'Category': 'DV_CATEGORY'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleId</th>\n",
              "      <th>FinalText</th>\n",
              "      <th>DV_CATEGORY</th>\n",
              "      <th>categoryId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1833</td>\n",
              "      <td>worldcom ex-boss launch defenc lawyer defend f...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>154</td>\n",
              "      <td>german busi confid slide german busi confid fe...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>bbc poll indic econom gloom citizen major nati...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1976</td>\n",
              "      <td>lifestyl govern mobil choic faster well funkie...</td>\n",
              "      <td>tech</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>917</td>\n",
              "      <td>enron bos $ m payout eighteen former enron dir...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1582</td>\n",
              "      <td>howard truant play snooker conserv leader mich...</td>\n",
              "      <td>politics</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>651</td>\n",
              "      <td>wale silent grand slam talk rhi william say wa...</td>\n",
              "      <td>sport</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1797</td>\n",
              "      <td>french honour director parker british film dir...</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2034</td>\n",
              "      <td>car giant hit merced slump slump profit luxuri...</td>\n",
              "      <td>business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1866</td>\n",
              "      <td>focker fuel festiv film chart comedi meet fock...</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ArticleId                                          FinalText  \\\n",
              "0       1833  worldcom ex-boss launch defenc lawyer defend f...   \n",
              "1        154  german busi confid slide german busi confid fe...   \n",
              "2       1101  bbc poll indic econom gloom citizen major nati...   \n",
              "3       1976  lifestyl govern mobil choic faster well funkie...   \n",
              "4        917  enron bos $ m payout eighteen former enron dir...   \n",
              "5       1582  howard truant play snooker conserv leader mich...   \n",
              "6        651  wale silent grand slam talk rhi william say wa...   \n",
              "7       1797  french honour director parker british film dir...   \n",
              "8       2034  car giant hit merced slump slump profit luxuri...   \n",
              "9       1866  focker fuel festiv film chart comedi meet fock...   \n",
              "\n",
              "     DV_CATEGORY  categoryId  \n",
              "0       business           0  \n",
              "1       business           0  \n",
              "2       business           0  \n",
              "3           tech           1  \n",
              "4       business           0  \n",
              "5       politics           2  \n",
              "6          sport           3  \n",
              "7  entertainment           4  \n",
              "8       business           0  \n",
              "9  entertainment           4  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq9flKMpBxKz",
        "outputId": "5dbb2dce-715c-42e6-d6f5-6a6da3081ff8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['business', 'tech', 'politics', 'sport', 'entertainment'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.DV_CATEGORY.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7c5YVYnTT5YX"
      },
      "outputs": [],
      "source": [
        "X_data = data[['FinalText']].to_numpy().reshape(-1)\n",
        "y_data = data[['DV_CATEGORY']].to_numpy().reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jjD9-_y-T5ab"
      },
      "outputs": [],
      "source": [
        "def bert_encode(texts, tokenizer):\n",
        "    ct = len(texts)\n",
        "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
        "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
        "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
        "\n",
        "    for k, text in enumerate(texts):\n",
        "        # Tokenize\n",
        "        tok_text = tokenizer.tokenize(str(text))\n",
        "\n",
        "        # Truncate and convert tokens to numerical IDs\n",
        "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
        "\n",
        "        input_length = len(enc_text) + 2\n",
        "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
        "\n",
        "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
        "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
        "\n",
        "        # Set to 1s in the attention input\n",
        "        attention_mask[k,:input_length] = 1\n",
        "\n",
        "    return {\n",
        "        'input_word_ids': input_ids,\n",
        "        'input_mask': attention_mask,\n",
        "        'input_type_ids': token_type_ids\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qdNSkLUT5cV",
        "outputId": "175a556f-7c8f-4841-ac99-2960bf41f810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'business', 1: 'tech', 2: 'politics', 3: 'sport', 4: 'entertainment'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform categories into numbers\n",
        "category_to_id = {}\n",
        "category_to_name = {}\n",
        "\n",
        "for index, c in enumerate(y_data):\n",
        "    if c in category_to_id:\n",
        "        category_id = category_to_id[c]\n",
        "    else:\n",
        "        category_id = len(category_to_id)\n",
        "        category_to_id[c] = category_id\n",
        "        category_to_name[category_id] = c\n",
        "\n",
        "    y_data[index] = category_id\n",
        "\n",
        "# Display dictionary\n",
        "category_to_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2HOoBzcqT5d8"
      },
      "outputs": [],
      "source": [
        "categories = data['DV_CATEGORY'].unique()\n",
        "n_categories = len(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Q2f0HPI7kN",
        "outputId": "b0b063e0-b528-44d1-ea12-867c36e58f76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oNcQ3GWVT5f-"
      },
      "outputs": [],
      "source": [
        "# Split into train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=777) # random_state to reproduce results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUeQhYKxIMMA",
        "outputId": "fbc8105c-dc0c-45d6-b51b-ab871f0c1c0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['game help learn play god game player must control virtual peopl societi could educ say research . u research suggest game sim could good way teach languag . ravi purushotma believ world sim well job teach vocabulari grammar tradit method . inher fun game play could help make learn languag much less chore say mr purushotma . must parent teacher worri lure video game comput consol hard resist child realli homework . instead fear comput game ravi purushotma believ educationalist particularli languag teacher embrac game . one goal would break believ fals assumpt learn play inher opposit say . believ phenomen abil game sim other captur interest adolesc audienc ripe exploit . hard part learn languag say mr purushotma basic part learn differ word refer use build sentenc . bore lesson drum vocabulari pupil couch term understand make mani languag far harder learn . way often teach foreign languag right somewhat akin learn ride bike formal studi graviti say . contrast say mr purushotma learn via someth like sim may mean student feel like studi . sim reli sole word get inform across player . instead action comput control peopl interact world often make clear go . incident inform sim could reinforc player student suppos learn say mr purushotma . contrast mani languag lesson tri impart inform tongu littl context . instanc say version sim adapt teach german player misunderstood meant word energi action tire sim stumbl fall asleep would illustr mean . necessari detail textual inform could call upon aid player student understand . one drawback sim say mr purushotma lack spoken languag help peopl brush pronunci . howev onlin version sim peopl move meet neighbour get know local town could adapt help . although wish claim first suggest use game help peopl learn mr purushotma believ educationalist miss potenti help . get simul person perform everyday activ make-believ world describ foreign languag could power learn aid believ . say educ softwar titl suffer comparison slick graphic rich world found game . say use pre-prepar game world sim never easy tool make creator fan make easi modifi almost part game . could make easi teacher adapt part game lesson . hope re-creat well-polish german learn mod sequel summer told bbc news websit . encourag hear other think experi japanes spanish . earlier work colleagu use civilis iii teach student histori show could power way get realis solv societi problem alway come make singl chang . report experi say : student begin ask histor geograph question context game play use geographi histori tool game draw infer social phenomenon base play . mr purushotma idea air articl journal languag learn technolog .',\n",
              "       'fox reliant realiti tv head u tv network fox admit broadcast reli heavili realiti tv show poor-rat daddi . chief execut gail berman say case fall drift much unscript side . seri daddi young woman tri pick natur father cash prize caus outrag adopt group rate badli . last season fox prime-tim audienc fell . million . m berman say : think audienc expect loud thing fox . sometim work sometim t. daddi first episod show januari pull disappoint audienc . million accord nielsen rate system . five episod show also film drop fox schedul m berman say . predict drop rate even network establish realiti show american idol due start fourth seri week . fox unveil new strategi last year promis launch new show everi season includ tradit quiet summer season . though met poor recept m berman say question audienc mind readi will abl accept new program summer . fox chang plan launch new show may instead june . one new show anim seri american dad make seth macfarlan creator famili guy . seri becom hit dvd also set return new episod .',\n",
              "       'cup holder man utd visit everton holder manchest unit premiership leader chelsea face difficult away tie premiership opposit fa cup fifth round . unit drawn everton chelsea face trip newcastl . brentford hartlepool - side left outsid top two divis - replay right travel southampton . burnley reward place last home tie lancashir rival blackburn . tie manchest unit everton could see return teenag striker wayn rooney former club first time sinc acrimoni £m move . nottingham forest bos gari megson could face trip back old club west brom come fourth-round replay tottenham . arsen hand potenti home tie fellow london west ham provid hammer come replay sheffield unit . charlton play leicest bolton await winner derby-fulham replay . : bolton v derbi fulham west bromwich albion tottenham v nottingham forest everton v manchest unit charlton athlet v leicest citi burnley v blackburn southampton v brentford hartlepool newcastl v chelsea arsen v west ham sheffield unit tie play / februari .',\n",
              "       ...,\n",
              "       'connor ralli cri british tenni heart much gut much hate lose question jimmi connor ask britain brightest tenni hope month possibl year come . american legend swept london thursday announc long-term relationship lawn tenni associ spend three day elit perform winter camp la manga . man epitomis phrase win lleyton hewitt even born clear qualiti hope convey . know everybodi hit ball well fine line number one number say connor . fine line long time . help want part teach kid win . connor first great name offer servic lta - long-tim rival john mcenro repeatedli do . connor pain point interest goe well beyond public stunt give glow recommend work lta perform director david felgat team . britain differ attitud right attitud take game forward find next wimbledon champion say connor . someth find everi day . everybodi talk good game everybodi put effect . impress come away see david coach three day one like never see especi . go happen overnight miracl worker go right direct . -year-old enthusiasm work go countri mark contrast relationship tenni offici back america . discus usta ( unit state tenni associ ) number year say . ( input ) end . connor straight-talk equal blunt honest deal lta cream britain young player . chosen get produc say . someon come take place . one number one spot lone get best view . year see andrew murray emerg britain great new hope connor reject suggest scot might put much pressur soon . tim henman whole countri shoulder say connor . know pressur like certain push get next level . someon go come take ( henman ) . andrew murray embrac . play . despit emphasi hard work train prepar connor admit desir requir champion come within . passion know find admit . also nut . say noth well compet tenni court . import thing world someth get right . noth well ever play tenni front peopl . play tenni . mould .',\n",
              "       'star pay tribut actor davi hollywood star includ spike lee burt reynold oscar nomine alan alda paid tribut actor ossi davi funer new york . veteran star ossi davi well-known civil right activist die miami age februari . friend famili includ actress rubi dee wife year gather riversid church saturday . also present servic former u presid bill clinton singer harri belafont give eulog . would good presid unit state say mr clinton . like give give . -year-old found dead last weekend hotel room florida make film . polic say appear die natur caus . davi make act debut way star sidney poiter . frequent collabor director spike lee star seven lee film includ jungl fever right thing malcolm x. attallah shabazz daughter activist malcolm x recal famou eulog deliv davi father funer . harlem come bid farewel one fine hope say quot man knew uncl ossi . ditto . ossi hero still say aviat star alan alda famili friend forti year . ossi thing beauti . want badli someday digniti - littl anyway ad burt reynold davi co-star s tv comedi even shade . midday funer score harlem resid form queue outsid church pay respect davi . hard fathom longer abl call wisdom humour loyalti moral strength guid u choic yet make battl yet fought say belafont ardent civil right activist friend davi year . fortun long .',\n",
              "       'europ back digit tv lifestyl peopl receiv digit entertain futur could chang follow launch ambiti european project . nice last week european commiss announc network & electron medium ( nem ) initi . broad scope stretch way medium creat stage distribut playback . commiss want peopl abl locat content desir deliv seamlessli move home work matter suppli devic network content content protect scheme . expert nice share vision interconnect futur hear pledg support compani nokia intel philip alcatel franc telecom thomson telefonica . might initi appear surpris compani direct competit keen work togeth . speaker state could see incompat stand-alon solut work . long-term strategi evolut converg technolog servic would requir . european commiss pragmat approach . identifi mani group defin form digit medium area nem encompass . nem approach take seriou look avail pipelin pick best bring togeth identifi gap . find hole develop standard fill . signific larg power organis state desir digit format open work gadget . bound plea surpris mani individu user organis feel wish holder right content normal consid consum . mani feel difficult challeng area commiss identifi solut differ digit right manag ( drm ) scheme . current drm solut incompat lock certain type purchas content make unplay platform . potenti percentag everi medium transact take place global prize supplier world domin drm scheme huge . although entertain obviou first step encompass remot provis healthcar energi effici control smart home . -year plan bring togeth work mani current run research project ec fund number year . simon perri editor digit lifestyl websit cover impact technolog medium'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-9YDnMKjT5iJ"
      },
      "outputs": [],
      "source": [
        "# Import tokenizer from HuggingFace\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "v-7RLCMqT5kA"
      },
      "outputs": [],
      "source": [
        "X_train = bert_encode(X_train, tokenizer)\n",
        "X_test = bert_encode(X_test, tokenizer)\n",
        "\n",
        "y_train = np.asarray(y_train, dtype='int32')\n",
        "y_test = np.asarray(y_test, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vOvA9Ns7UZIY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def build_model(n_categories):\n",
        "    with strategy.scope():\n",
        "        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
        "        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
        "\n",
        "        # Import BERT model from HuggingFace\n",
        "        bert_model = TFBertModel.from_pretrained(MODEL_NAME)\n",
        "        x = bert_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
        "\n",
        "        # Huggingface transformers have multiple outputs, embeddings are the first one,\n",
        "        # so let's slice out the first position\n",
        "        x = x[0]\n",
        "\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, n_categories, model_name='bert-base-uncased'):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size * MAX_LEN, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, n_categories)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        # Get embeddings from BERT\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        x = outputs[0]  # Extract the last hidden state\n",
        "        \n",
        "        # Flatten and pass through the network\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERTClassifier(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=196608, out_features=256, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=256, out_features=5, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "with strategy.scope():\n",
        "    model = build_model(n_categories)\n",
        "    model.summary()\n",
        "\"\"\"\n",
        "\n",
        "# Example of model creation\n",
        "model = BERTClassifier(n_categories=n_categories)\n",
        "\n",
        "# Move model to device (GPU/CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Example of model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: <class 'dict'> dict_keys(['input_word_ids', 'input_mask', 'input_type_ids'])\n",
            "y_train: <class 'numpy.ndarray'> (1043,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train:\", type(X_train), X_train.keys())\n",
        "print(\"y_train:\", type(y_train), y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example assuming the data is in a specific key\n",
        "# Adjust 'input_ids' to the appropriate key based on your data\n",
        "X_train_word_ids = X_train['input_word_ids']\n",
        "X_test_word_ids = X_test['input_word_ids']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJtUEjSvUZOi",
        "outputId": "767b4a86-5023-4ff4-d358-fce3fd474684"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "with strategy.scope():\n",
        "    print('Training...')\n",
        "    history = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs=EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, y_test))\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Move data to the device (GPU/CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming X_train, y_train, X_test, and y_test are numpy arrays\n",
        "#X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
        "X_train_tensor = torch.tensor(X_train_word_ids, dtype=torch.long).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "#X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
        "X_test_tensor = torch.tensor(X_test_word_ids, dtype=torch.long).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch [1/3], Loss: 1.2254\n",
            "Validation Accuracy after Epoch 1: 0.9463\n",
            "Epoch [2/3], Loss: 0.9801\n",
            "Validation Accuracy after Epoch 2: 0.9418\n",
            "Epoch [3/3], Loss: 0.9618\n",
            "Validation Accuracy after Epoch 3: 0.9485\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoader for training and validation\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = BERTClassifier(n_categories=n_categories).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "print('Training...')\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids, labels = batch\n",
        "        \n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, \n",
        "                        attention_mask=input_ids.ne(0).long(),  # attention mask\n",
        "                        token_type_ids=torch.zeros_like(input_ids).long())  # token type ids\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, labels = batch\n",
        "            outputs = model(input_ids=input_ids, \n",
        "                            attention_mask=input_ids.ne(0).long(), \n",
        "                            token_type_ids=torch.zeros_like(input_ids).long())\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, all_preds)\n",
        "    print(f'Validation Accuracy after Epoch {epoch+1}: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "130Vi7HuKj22",
        "outputId": "e53deb68-38ec-4168-8dc0-0522d322f66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.91      0.94      0.93       100\n",
            "         tech       1.00      0.89      0.94        83\n",
            "     politics       0.89      0.94      0.91        82\n",
            "        sport       0.97      1.00      0.99       108\n",
            "entertainment       0.99      0.96      0.97        74\n",
            "\n",
            "     accuracy                           0.95       447\n",
            "    macro avg       0.95      0.95      0.95       447\n",
            " weighted avg       0.95      0.95      0.95       447\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, all_preds, target_names=categories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating confusion matrix...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix saved as ../../out/bbc_news_bert_confusion_matrix.png\n"
          ]
        }
      ],
      "source": [
        "create_confusion_matrix(\n",
        "            y_test, \n",
        "            all_preds, \n",
        "            title='Confusion Matrix for Bert BBC-News Classification Model',\n",
        "            file_name=OUT_DIR+'bbc_news_bert_confusion_matrix.png',\n",
        "            debug=False\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
