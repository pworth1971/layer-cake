# -*- coding: utf-8 -*-
"""20NewsGroup-SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gf-kbOARNTSugpoLKkmeorN6_x2iej9o
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import re
import string

import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

from timeit import default_timer as timer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import model_selection, svm, metrics
from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, hamming_loss, jaccard_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
from sklearn.metrics import make_scorer
from sklearn.metrics import recall_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_20newsgroups

from pprint import pprint

import re
import string

import cufflinks as cf
import plotly.offline as pyo
import plotly.graph_objs as go

import numpy as np
from matplotlib import pyplot as plt

import warnings
warnings.filterwarnings('ignore')


nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')


# Fetch the 20 newsgroups dataset
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))

# Create a DataFrame from the Bunch object
df = pd.DataFrame({
    'text': newsgroups.data,
    'category': newsgroups.target
})

# Add category names
df['category_name'] = [newsgroups.target_names[i] for i in df['category']]


print(f"Number of documents: {len(df)}")
print(f"Number of categories: {len(df['category'].unique())}")
print(f"Number of category names: {len(df['category_name'].unique())}")
#pprint(list(df.target_names))
#pprint(list(df.category_name))

#df = df['category'].unique()

def missing_values(df):
    """
    Calculate the percentage of missing values for each column in a DataFrame.
    
    Args:
    df (pd.DataFrame): The input DataFrame to analyze.
    
    Returns:
    pd.DataFrame: A DataFrame containing the total count and percentage of missing values for each column.
    """
    # Calculate total missing values and their percentage
    total = df.isnull().sum()
    percent = (total / len(df) * 100)
    
    # Create a DataFrame with the results
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
    
    # Sort the DataFrame by percentage of missing values (descending)
    missing_data = missing_data.sort_values('Percent', ascending=False)
    
    # Filter out columns with no missing values
    missing_data = missing_data[missing_data['Total'] > 0]
    
    print("Columns with missing values:")
    print(missing_data)
    
    return missing_data

missing_values_df = missing_values(df)
print(f"missing values:", missing_values_df)

### Start of Text Pre-processing
print("preproccessing...")

string.punctuation

### 2. To LowerCase

df['CleanedText'] = (df.text.apply(lambda x: x.lower()))

### 3. Removing Numbers and Special Characters including XXXXXX

df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('\W+', ' ', x)))
regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')

df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub(regex, '', x)))
df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('xxxx', '', x)))
df['CleanedText'] =  (df.CleanedText.apply(lambda x: re.sub('xx', '', x)))

### 4. Removing Punctuation

def remove_punctuation(x):
    punctuationfree="".join([i for i in x if i not in string.punctuation])
    return punctuationfree

df['CleanedText'] =  (df.CleanedText.apply(lambda x: remove_punctuation(x)))

### 5. Tokenization
#data['TokenizedText'] =  (data.CleanedText.apply(lambda x: re.split('W+',x)))


### 6. Removing Stopwords

nltk.download('stopwords')
stopwords = set(stopwords.words("english"))
df['CleanedText'] = df.CleanedText.apply(lambda x: " ".join(x for x in x.split() if x not in stopwords))
print("Stopwords removed")
#print(df['CleanedText'][0])

 ## TFIDF already tokenizes the text so no need to tokenize it here
# from nltk.tokenize import sent_tokenize, word_tokenize
# data2['TokenizedText'] = data2.CleanedText.apply(word_tokenize)


### 7. Text Normalization  [Lemmatization] -->better than Stemming since it returns actual words
## lemmatization is an intelligent operation that uses dictionaries

# Using Spacy
# import spaCy's language model
# function to lemmatize text

print("Lemmatizing...")

# Function to lemmatize text with memory optimization
def lemmatization(texts, chunk_size=1000):
    lmtzr = WordNetLemmatizer()
    
    num_chunks = len(texts) // chunk_size + 1
    print(f"Number of chunks: {num_chunks}")
    for i in range(num_chunks):
        chunk = texts[i*chunk_size:(i+1)*chunk_size]
        texts[i*chunk_size:(i+1)*chunk_size] = [' '.join([lmtzr.lemmatize(word) for word in text.split()]) for text in chunk]
    
    return texts


"""
lmtzr = WordNetLemmatizer()
def lemmatization(texts):   ## Stemming (Pls read the difference)
    output = []
    for x in texts:
        s = [lmtzr.lemmatize(w) for w in df['CleanedText']]
        output.append(' '.join(s))
    return output
"""

df['LemmatizedText'] = lemmatization(df['CleanedText'])

print("Lemmatized")
#print(df['CleanedText'][0])
#print(df['LemmatizedText'][0])

"""
def tokenize(text):
    '''
    Tokenize text and return a non-unique list of tokenized words found in the text.
    Normalize to lowercase, strip punctuation, remove stop words, filter non-ascii characters.
    Lemmatize the words and lastly drop words of length < 3.
    '''
    text = text.lower()
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')
    nopunct = regex.sub(" ", text)
    words = nopunct.split(' ')
    # remove any non ascii
    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]
    lmtzr = WordNetLemmatizer()
    words = [lmtzr.lemmatize(w) for w in words]
    #words = [w for w in words if len(w) > 2]
    return words
"""
# POS Tagging and Counting
print("Tokenizing...")

df['tokenized'] = df['text'].str.lower().apply(nltk.word_tokenize)
tagged_titles = df['text'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))

def count_tags(title_with_tags):
    tag_count = {}
    for word, tag in title_with_tags:
        tag_count[tag] = tag_count.get(tag, 0) + 1
    return tag_count

# Create a DataFrame with POS tag counts
tagged_titles_df = pd.DataFrame(tagged_titles.apply(lambda x: count_tags(x)).tolist()).fillna(0)

# Sum the occurrences of each tag across all documents
tagged_titles_sum = tagged_titles_df.sum().sort_values(ascending=False)

"""
# Plot POS Tag Frequency
trace = go.Bar(x=tagged_titles_sum.index, y=tagged_titles_sum.values)
layout = go.Layout(title='Frequency of POS Tags in IT Support Tickets Dataset', xaxis=dict(title='POS'), yaxis=dict(title='Count'))
fig = go.Figure(data=[trace], layout=layout)

# This will open the plot in the default web browser
pyo.plot(fig, filename='../../out/pos_tag_frequency.html')
"""

# Feature Extraction and Model Training
print("Splitting the dataset...")

X = df['CleanedText']
y = df['category']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44)

print("Vectorizing...")

vectorizer = TfidfVectorizer(ngram_range=(1,3), sublinear_tf=True, use_idf=True)
X_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print("Fitting the model...")

svm = LinearSVC(class_weight='balanced', max_iter=1000)
clf = svm.fit(X_tfidf, y_train)

y_pred = model_selection.cross_val_predict(svm, X_test_tfidf, y_test, cv=10)
print("\n\nAccuracy for SVM :", metrics.accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

print("Using GridSearchCV...")
svm_classifier = LinearSVC(class_weight='balanced', max_iter=1000)

parameter_grid = {
    'class_weight': [None, 'balanced'],
    'C': np.logspace(-3, 3, 7)
    }

cross_validation = StratifiedKFold()

#scoring = ['accuracy', 'precision', 'recall', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted']
#from sklearn.metrics import accuracy_score, f1_score, fbeta_score, recall_score, precision_score, hamming_loss, jaccard_score, 

scorers = {
            'accuracy_score': make_scorer(accuracy_score),
            'f1_score': make_scorer(f1_score, average='micro'),
            'recall_score': make_scorer(recall_score, average='micro'),
            'precision_score': make_scorer(precision_score, average='micro'),
            'hamming_loss': make_scorer(hamming_loss),
            'jaccard_score': make_scorer(jaccard_score, average='micro')
          }

grid_search = GridSearchCV(
    n_jobs=-1, 
    estimator=svm_classifier,
    refit='f1_score',
    param_grid=parameter_grid,
    cv=cross_validation,
    #scoring=scoring
    scoring=scorers,
    return_train_score=True         # ensure train scores are calculated
    )

grid_search.fit(X_tfidf, y_train)

print('Best parameters: {}'.format(grid_search.best_params_))
print("best_estimator:", grid_search.best_estimator_)
print('Best score: {}'.format(grid_search.best_score_))
print("cv_results_:", grid_search.cv_results_)

results = grid_search.cv_results_


print("Plotting the results...")

# Define the metrics we want to plot
metrics_to_plot = ['accuracy_score', 'f1_score', 'recall_score', 'precision_score', 'hamming_loss']

# Iterate over each metric to create a separate plot
for metric in metrics_to_plot:
    traces = []

    print(f"Plotting {metric}...")

    for sample in ["train", "test"]:

        key_mean = f"mean_{sample}_{metric}"
        key_std = f"std_{sample}_{metric}"

        print(f"Plotting {key_mean}...")
        print(f"Plotting {key_std}...")

        # Directly use the keys without conditional check
        sample_score_mean = np.nan_to_num(np.array(results[key_mean]) * 100)  # Convert to percentage and handle NaN
        sample_score_std = np.nan_to_num(np.array(results[key_std]) * 100)  # Convert to percentage and handle NaN

        x_axis = np.linspace(0, 100, len(sample_score_mean))

        # Create the trace for Plotly
        traces.append(
            go.Scatter(
                x=x_axis,
                y=sample_score_mean,
                mode='lines+markers',
                name=f"{metric} ({sample})",
                line=dict(dash='dash' if sample == 'train' else 'solid'),
                error_y=dict(
                    type='data',
                    array=sample_score_std,
                    visible=True
                ),
                hoverinfo='x+y+name'
            )
        )

    # Define the layout of the plot
    layout = go.Layout(
        title={'text': f"Training and Test Scores for {metric.capitalize()}",
               'y':0.9,
               'x':0.5,
               'xanchor': 'center',
               'yanchor': 'top'},
        xaxis=dict(title="Training Sample Percentage (%)"),
        yaxis=dict(title="Score (%)", range=[0, 100]),
        hovermode='closest'
    )

    # Create the figure
    fig = go.Figure(data=traces, layout=layout)

    # Write the plot to an HTML file
    filename = f'../../out/training_test_scores_{metric}.html'
    pyo.plot(fig, filename=filename)

    print(f"Saved plot for {metric} as {filename}")